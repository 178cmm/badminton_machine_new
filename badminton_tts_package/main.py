"""
ğŸ¸ ç¾½çƒç™¼çƒæ©ŸèªéŸ³æ§åˆ¶ç³»çµ±ï¼ˆASR â†’ LLM â†’ TTS ä¸€æ¢é¾ï¼‰

æµç¨‹ï¼š
1) VAD è‡ªå‹•éŒ„éŸ³ï¼ˆåµæ¸¬èªéŸ³æ´»å‹•ï¼‰
2) Whisper API èªéŸ³è­˜åˆ¥ï¼ˆé«˜æº–ç¢ºåº¦ï¼‰
3) ç¾½çƒè¦å‰‡åŒ¹é…ï¼šå‘½ä¸­å‰‡ç›´æ¥å›è¦†ï¼Œè·³é LLM
4) è‹¥è¦å‰‡æœªå‘½ä¸­ï¼Œå°‡è½‰éŒ„çµæœé€å…¥ LLM ç”Ÿæˆå›è¦†
5) TTS èªéŸ³åˆæˆå›è¦†ï¼Œè‡ªå‹•æ’­æ”¾

ä¾è³´ï¼šopenai, sounddevice, scipyï¼Œï¼ˆå»ºè­°ï¼‰opencc-python-reimplemented, pydubï¼ˆèªé€Ÿèª¿æ•´ï¼‰, webrtcvadï¼ˆVAD æ¨¡å¼ï¼‰
è¦å‰‡ç³»çµ±ï¼špyyaml, rapidfuzz
ç’°å¢ƒè®Šæ•¸ï¼šOPENAI_API_KEY

ç”¨æ³•ï¼š
  ğŸ¸ ç¾½çƒç™¼çƒæ©Ÿå®Œæ•´ç‰ˆï¼ˆæ¨è–¦ï¼‰ï¼š
    python main.py

  âš¡ å³æ™‚è½‰éŒ„æ¨¡å¼ï¼ˆèªªè©±æ™‚å³æ™‚é¡¯ç¤ºæ–‡å­—ï¼‰ï¼š
    python main.py --realtime

  ğŸ” å¤šå›åˆè¨“ç·´æ¨¡å¼ï¼š
    python main.py --loop

  ğŸ›ï¸ é€²éšé¸é …ï¼š
    python main.py -v onyx -o reply.mp3 --speed 1.5

è¦å‰‡ç³»çµ±ç‰¹è‰²ï¼š
- æ”¯æ´ containsï¼ˆåŒ…å«è©ï¼‰ã€regexï¼ˆæ­£å‰‡ï¼‰ã€fuzzyï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
- å„ªå…ˆåºæ§åˆ¶ï¼ˆpriority è¶Šé«˜è¶Šå„ªå…ˆï¼‰
- å‹•æ…‹è®Šæ•¸ï¼ˆå¦‚ {balls_left} å‰©é¤˜çƒæ•¸ï¼‰
- ç†±é‡è¼‰ï¼ˆä¿®æ”¹è¦å‰‡æª”ç«‹å³ç”Ÿæ•ˆï¼‰
- å¯å°æ¥å¯¦é«”å‹•ä½œï¼ˆç™¼çƒæ©Ÿ APIï¼‰

Whisper API ç‰¹è‰²ï¼š
- é«˜æº–ç¢ºåº¦èªéŸ³è­˜åˆ¥
- æ”¯æ´å¤šç¨®èªè¨€
- ç„¡éœ€æœ¬åœ°æ¨¡å‹æª”æ¡ˆ
- è‡ªå‹•èªè¨€åµæ¸¬
- æ›´å¥½çš„ä¸­æ–‡è­˜åˆ¥æ•ˆæœ
"""

import argparse
import pathlib
import subprocess
import sys
import os
import json
import re
import time
import logging
import threading
import concurrent.futures
from typing import Optional
from dataclasses import dataclass, field

import numpy as np
import sounddevice as sd
from scipy.io.wavfile import write as wavwrite, read as wavread
import shutil
import subprocess
from openai import OpenAI

try:
    # ç¹é«”è½‰æ›ï¼ˆs2twpï¼šè‡ºç£æ…£ç”¨è©ï¼‰
    from opencc import OpenCC  # type: ignore
    _cc = OpenCC('s2twp')
except Exception:
    _cc = None

try:
    # èªé€Ÿèª¿æ•´
    from pydub import AudioSegment
    from pydub.effects import speedup
    PYDUB_AVAILABLE = True
except ImportError:
    PYDUB_AVAILABLE = False

try:
    import webrtcvad
    WEBRTCVAD_AVAILABLE = True
except ImportError:
    WEBRTCVAD_AVAILABLE = False

try:
    # è¦å‰‡ç³»çµ±ä¾è³´
    import yaml
    from rapidfuzz import fuzz
    RULES_AVAILABLE = True
except ImportError:
    RULES_AVAILABLE = False


# === é…ç½®ç®¡ç† ===
@dataclass
class AudioConfig:
    """éŸ³è¨Šé…ç½®"""
    sample_rate: int = 16000
    channels: int = 1
    frame_duration_ms: int = 30
    min_speech_frames: int = 10
    max_recording_ms: int = 60000
    silence_ms: int = 300
    aggressiveness: int = 2
    max_buffer_frames: int = 1000  # æœ€å¤§ç·©è¡å€å¹€æ•¸
    buffer_cleanup_threshold: int = 500  # ç·©è¡å€æ¸…ç†é–¾å€¼
    # ä½å»¶é²å„ªåŒ–
    fast_silence_ms: int = 400  # å¿«é€Ÿæ¨¡å¼éœéŸ³åµæ¸¬æ™‚é–“
    ultra_fast_silence_ms: int = 200  # è¶…å¿«é€Ÿæ¨¡å¼éœéŸ³åµæ¸¬æ™‚é–“
    min_speech_frames_fast: int = 5  # å¿«é€Ÿæ¨¡å¼æœ€å°‘èªéŸ³å¹€æ•¸

@dataclass
class PreloadConfig:
    """é è¼‰å…¥é…ç½®"""
    enabled: bool = True  # å•Ÿç”¨é è¼‰å…¥
    max_cache_size: int = 50  # æœ€å¤§å¿«å–æ•¸é‡
    preload_common_replies: bool = True  # é è¼‰å…¥å¸¸ç”¨å›è¦†
    prediction_enabled: bool = True  # å•Ÿç”¨é æ¸¬é‚è¼¯
    cache_ttl: int = 3600  # å¿«å–å­˜æ´»æ™‚é–“ï¼ˆç§’ï¼‰
    hot_reload_threshold: int = 3  # ç†±é»é‡æ–°è¼‰å…¥é–¾å€¼
    # æŒä¹…åŒ–å¿«å–é…ç½®
    persistent_cache: bool = True  # å•Ÿç”¨æŒä¹…åŒ–å¿«å–
    cache_file: str = "cache/reply_cache.json"  # å¿«å–æª”æ¡ˆè·¯å¾‘
    auto_save_interval: int = 300  # è‡ªå‹•å„²å­˜é–“éš”ï¼ˆç§’ï¼‰
    # è¦å‰‡å¿«å–é…ç½®
    rule_cache_enabled: bool = True  # å•Ÿç”¨è¦å‰‡å¿«å–
    rule_cache_ttl: int = 300  # è¦å‰‡å¿«å–å­˜æ´»æ™‚é–“ï¼ˆç§’ï¼‰
    preload_rules: bool = True  # é è¼‰å…¥è¦å‰‡åŒ¹é…

@dataclass
class AppConfig:
    """æ‡‰ç”¨ç¨‹å¼é…ç½®"""
    audio: AudioConfig = field(default_factory=AudioConfig)
    default_voice: str = "alloy"
    default_speed: float = 1.2
    max_conversation_history: int = 20
    max_retries: int = 3
    retry_delay: float = 1.0
    # ä½å»¶é²å„ªåŒ–
    low_latency_mode: bool = False  # å•Ÿç”¨ä½å»¶é²æ¨¡å¼
    skip_progress_indicators: bool = False  # è·³éé€²åº¦æŒ‡ç¤ºå™¨
    parallel_processing: bool = False  # å•Ÿç”¨ä¸¦è¡Œè™•ç†
    # é è¼‰å…¥å„ªåŒ–
    preload: PreloadConfig = field(default_factory=PreloadConfig)

# å…¨åŸŸé…ç½®å¯¦ä¾‹
app_config = AppConfig()

# å‘å¾Œç›¸å®¹çš„å¸¸æ•¸
SAMPLE_RATE = app_config.audio.sample_rate
CHANNELS = app_config.audio.channels

# === æ—¥èªŒè¨­å®š ===
def setup_logging(level: str = "INFO"):
    """è¨­å®šæ—¥èªŒç³»çµ±"""
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('badminton_tts.log'),
            logging.StreamHandler()
        ]
    )

# === é€²åº¦æŒ‡ç¤ºå™¨ ===
def show_progress(message: str, duration: float = 0.5, show_dots: bool = True):
    """é¡¯ç¤ºé€²åº¦æŒ‡ç¤º"""
    if app_config.skip_progress_indicators:
        print(f"â³ {message}")
        return
    
    if show_dots:
        print(f"â³ {message}", end="", flush=True)
        time.sleep(duration)
        print(" âœ…")
    else:
        print(f"â³ {message}")


def show_fast_progress(message: str):
    """å¿«é€Ÿé€²åº¦æŒ‡ç¤ºï¼ˆç„¡å»¶é²ï¼‰"""
    print(f"âš¡ {message}")


def show_progress_with_dots(message: str, total_steps: int = 3):
    """é¡¯ç¤ºå¸¶é»é»çš„é€²åº¦æŒ‡ç¤º"""
    if app_config.skip_progress_indicators:
        print(f"âš¡ {message}")
        return
    
    print(f"â³ {message}", end="", flush=True)
    for i in range(total_steps):
        time.sleep(0.3)
        print(".", end="", flush=True)
    print(" âœ…")


def show_loading_bar(message: str, duration: float = 2.0, width: int = 20):
    """é¡¯ç¤ºè¼‰å…¥é€²åº¦æ¢"""
    print(f"â³ {message}")
    for i in range(width + 1):
        progress = i / width
        bar = "â–ˆ" * i + "â–‘" * (width - i)
        print(f"\r[{bar}] {progress*100:.0f}%", end="", flush=True)
        time.sleep(duration / width)
    print()  # æ›è¡Œ

# === è¨˜æ†¶é«”å„ªåŒ– ===
def _optimize_audio_buffer(audio_buffer: list, max_frames: Optional[int] = None) -> list:
    """å„ªåŒ–éŸ³è¨Šç·©è¡å€è¨˜æ†¶é«”ä½¿ç”¨"""
    if max_frames is None:
        max_frames = app_config.audio.max_buffer_frames
    
    if len(audio_buffer) > max_frames:
        # åªä¿ç•™æœ€è¿‘çš„éŸ³è¨Šå¹€ï¼Œé‡‹æ”¾èˆŠçš„è¨˜æ†¶é«”
        return audio_buffer[-max_frames:]
    return audio_buffer


def _cleanup_old_frames(audio_buffer: list, threshold: Optional[int] = None) -> list:
    """æ¸…ç†èˆŠçš„éŸ³è¨Šå¹€ä»¥é‡‹æ”¾è¨˜æ†¶é«”"""
    if threshold is None:
        threshold = app_config.audio.buffer_cleanup_threshold
    
    if len(audio_buffer) > threshold:
        # ä¿ç•™æœ€è¿‘çš„å¹€ï¼Œæ¸…ç†èˆŠçš„
        return audio_buffer[-threshold:]
    return audio_buffer


def _get_memory_usage() -> float:
    """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    try:
        import psutil
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024  # è½‰æ›ç‚º MB
    except ImportError:
        return 0.0


def _log_memory_usage(operation: str):
    """è¨˜éŒ„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼ˆå·²åœç”¨ï¼‰"""
    pass  # ä¸å†è¼¸å‡ºè¨˜æ†¶é«”è³‡è¨Š

# === Rules loader & matcher ===
_RULES_CACHE = {"path": None, "mtime": 0.0, "data": None, "compiled_regex": {}}

# === é è¼‰å…¥å›è¦†æ¨¡æ¿ç³»çµ± ===
class ReplyTemplateCache:
    """å›è¦†æ¨¡æ¿å¿«å–ç³»çµ±ï¼ˆæ”¯æ´æŒä¹…åŒ–ï¼‰"""
    
    def __init__(self):
        self.cache = {}  # {query_hash: {"reply": str, "timestamp": float, "count": int}}
        self.common_templates = {}  # å¸¸ç”¨å›è¦†æ¨¡æ¿
        self.prediction_queue = []  # é æ¸¬ä½‡åˆ—
        self.last_save_time = time.time()  # ä¸Šæ¬¡å„²å­˜æ™‚é–“
        self.rule_cache = {}  # è¦å‰‡åŒ¹é…çµæœå¿«å–
        self._load_common_templates()
        self._load_persistent_cache()
    
    def _load_common_templates(self):
        """è¼‰å…¥å¸¸ç”¨å›è¦†æ¨¡æ¿"""
        self.common_templates = {
            # ç¾½çƒç›¸é—œå¸¸ç”¨å›è¦†
            "é–‹å§‹": ["å¥½çš„ï¼Œæˆ‘å€‘é–‹å§‹ç·´ç¿’å§ï¼", "æº–å‚™å¥½äº†å—ï¼Ÿé–‹å§‹ç™¼çƒï¼", "é–‹å§‹è¨“ç·´ï¼"],
            "åœæ­¢": ["å¥½çš„ï¼Œåœæ­¢ç™¼çƒã€‚", "è¨“ç·´çµæŸï¼", "åœæ­¢ç™¼çƒæ©Ÿã€‚"],
            "é€Ÿåº¦": ["èª¿æ•´ç™¼çƒé€Ÿåº¦", "é€Ÿåº¦è¨­å®šå®Œæˆ", "ç™¼çƒé€Ÿåº¦å·²èª¿æ•´"],
            "è§’åº¦": ["èª¿æ•´ç™¼çƒè§’åº¦", "è§’åº¦è¨­å®šå®Œæˆ", "ç™¼çƒè§’åº¦å·²èª¿æ•´"],
            "çƒæ•¸": ["å‰©é¤˜çƒæ•¸", "çƒæ•¸çµ±è¨ˆ", "ç™¼çƒæ•¸é‡"],
            "å¹«åŠ©": ["æˆ‘å¯ä»¥å¹«ä½ æ§åˆ¶ç™¼çƒæ©Ÿ", "éœ€è¦ä»€éº¼å¹«åŠ©å—ï¼Ÿ", "æœ‰ä»€éº¼å•é¡Œå—ï¼Ÿ"],
            "è¬è¬": ["ä¸å®¢æ°£ï¼", "å¾ˆé«˜èˆˆèƒ½å¹«åŠ©ä½ ", "éš¨æ™‚ç‚ºä½ æœå‹™"],
            "ä½ å¥½": ["ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ç¾½çƒç™¼çƒæ©ŸåŠ©ç†", "å—¨ï¼æº–å‚™é–‹å§‹è¨“ç·´å—ï¼Ÿ", "ä½ å¥½ï¼ä»Šå¤©æƒ³ç·´ä»€éº¼ï¼Ÿ"],
            
            # é€šç”¨å›è¦†
            "ä¸çŸ¥é“": ["æˆ‘ä¸å¤ªç¢ºå®šï¼Œè®“æˆ‘å†æƒ³æƒ³", "é€™å€‹å•é¡Œæœ‰é»é›£ï¼Œè®“æˆ‘æŸ¥ä¸€ä¸‹", "æˆ‘éœ€è¦æ›´å¤šè³‡è¨Š"],
            "é‡è¤‡": ["è«‹å†èªªä¸€é", "æˆ‘æ²’è½æ¸…æ¥š", "å¯ä»¥é‡è¤‡ä¸€æ¬¡å—ï¼Ÿ"],
            "ç¢ºèª": ["å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†", "æ”¶åˆ°ï¼", "ç¢ºèªå®Œæˆ"],
            "å–æ¶ˆ": ["å·²å–æ¶ˆ", "æ“ä½œå–æ¶ˆ", "å–æ¶ˆå®Œæˆ"],
        }
    
    def cache_rule_result(self, query: str, rule_result: dict):
        """å¿«å–è¦å‰‡åŒ¹é…çµæœ"""
        if not (app_config.preload.enabled and app_config.preload.rule_cache_enabled):
            return
        
        query_hash = self._hash_query(query)
        self.rule_cache[query_hash] = {
            "rule": rule_result,
            "timestamp": time.time(),
            "count": 1
        }
    
    def get_cached_rule_result(self, query: str) -> Optional[dict]:
        """ç²å–å¿«å–çš„è¦å‰‡åŒ¹é…çµæœ"""
        if not (app_config.preload.enabled and app_config.preload.rule_cache_enabled):
            return None
        
        query_hash = self._hash_query(query)
        if query_hash in self.rule_cache:
            cached = self.rule_cache[query_hash]
            # æª¢æŸ¥å¿«å–æ˜¯å¦éæœŸ
            if time.time() - cached["timestamp"] < app_config.preload.rule_cache_ttl:
                cached["count"] += 1
                return cached["rule"]
            else:
                # éæœŸå‰‡ç§»é™¤
                del self.rule_cache[query_hash]
        
        return None
    
    def _hash_query(self, query: str) -> str:
        """ç”ŸæˆæŸ¥è©¢çš„é›œæ¹Šå€¼"""
        import hashlib
        normalized = _normalize_zh(query)
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def get_cached_reply(self, query: str) -> Optional[str]:
        """ç²å–å¿«å–çš„å›è¦†"""
        if not app_config.preload.enabled:
            return None
        
        query_hash = self._hash_query(query)
        if query_hash in self.cache:
            cached = self.cache[query_hash]
            # æª¢æŸ¥å¿«å–æ˜¯å¦éæœŸ
            if time.time() - cached["timestamp"] < app_config.preload.cache_ttl:
                cached["count"] += 1
                return cached["reply"]
            else:
                # éæœŸå‰‡ç§»é™¤
                del self.cache[query_hash]
        
        return None
    
    
    def _cleanup_cache(self):
        """æ¸…ç†å¿«å–ï¼ˆç§»é™¤æœ€å°‘ä½¿ç”¨çš„é …ç›®ï¼‰"""
        if not self.cache:
            return
        
        # æŒ‰ä½¿ç”¨æ¬¡æ•¸æ’åºï¼Œç§»é™¤æœ€å°‘ä½¿ç”¨çš„
        sorted_items = sorted(self.cache.items(), key=lambda x: x[1]["count"])
        # ç§»é™¤æœ€èˆŠçš„ 25%
        remove_count = max(1, len(sorted_items) // 4)
        for key, _ in sorted_items[:remove_count]:
            del self.cache[key]
    
    def predict_and_preload(self, current_query: str, conversation_history: list):
        """é æ¸¬å¯èƒ½çš„å¾ŒçºŒå•é¡Œä¸¦é è¼‰å…¥"""
        if not app_config.preload.prediction_enabled:
            return
        
        # åŸºæ–¼ç•¶å‰æŸ¥è©¢å’Œå°è©±æ­·å²é æ¸¬å¯èƒ½çš„å¾ŒçºŒå•é¡Œ
        predictions = self._generate_predictions(current_query, conversation_history)
        
        # å°‡é æ¸¬åŠ å…¥ä½‡åˆ—
        for prediction in predictions:
            if prediction not in self.prediction_queue:
                self.prediction_queue.append(prediction)
    
    def _generate_predictions(self, current_query: str, conversation_history: list) -> list:
        """ç”Ÿæˆé æ¸¬æŸ¥è©¢ï¼ˆåŸºæ–¼è¦å‰‡ç³»çµ±ï¼‰"""
        predictions = []
        
        # åŸºæ–¼é—œéµè©é æ¸¬
        query_lower = current_query.lower()
        
        # ç¾½çƒè¨“ç·´ç›¸é—œé æ¸¬
        if "é–‹å§‹" in query_lower or "start" in query_lower:
            predictions.extend(["åœæ­¢", "å¿«é€Ÿ", "æ…¢é€Ÿ", "å‰å ´", "å¾Œå ´", "æ®ºçƒ"])
        elif "åœæ­¢" in query_lower or "stop" in query_lower:
            predictions.extend(["é–‹å§‹", "ç‹€æ…‹", "çƒæ•¸"])
        elif "é€Ÿåº¦" in query_lower or "speed" in query_lower or "å¿«" in query_lower or "æ…¢" in query_lower:
            predictions.extend(["è§’åº¦", "é–‹å§‹", "åœæ­¢", "å·¦é‚Š", "å³é‚Š"])
        elif "è§’åº¦" in query_lower or "angle" in query_lower or "å·¦" in query_lower or "å³" in query_lower:
            predictions.extend(["é€Ÿåº¦", "é–‹å§‹", "åœæ­¢", "æé«˜", "é™ä½"])
        elif "çƒæ•¸" in query_lower or "ball" in query_lower or "å‰©é¤˜" in query_lower:
            predictions.extend(["é–‹å§‹", "åœæ­¢", "ç‹€æ…‹"])
        elif "å‰å ´" in query_lower or "ç¶²å‰" in query_lower:
            predictions.extend(["å¾Œå ´", "æ®ºçƒ", "åŠçƒ", "åœæ­¢"])
        elif "å¾Œå ´" in query_lower or "åº•ç·š" in query_lower:
            predictions.extend(["å‰å ´", "æ®ºçƒ", "åŠçƒ", "åœæ­¢"])
        elif "æ®ºçƒ" in query_lower or "æ‰£æ®º" in query_lower:
            predictions.extend(["åŠçƒ", "å‰å ´", "å¾Œå ´", "åœæ­¢"])
        elif "åŠçƒ" in query_lower or "è¼•åŠ" in query_lower:
            predictions.extend(["æ®ºçƒ", "å‰å ´", "å¾Œå ´", "åœæ­¢"])
        
        # åŸºæ–¼å°è©±æ­·å²é æ¸¬
        if conversation_history:
            last_reply = conversation_history[-1].get("content", "").lower()
            if "é–‹å§‹" in last_reply:
                predictions.extend(["åœæ­¢", "å¿«é€Ÿ", "æ…¢é€Ÿ", "å‰å ´", "å¾Œå ´"])
            elif "é€Ÿåº¦" in last_reply or "å¿«" in last_reply or "æ…¢" in last_reply:
                predictions.extend(["è§’åº¦", "é–‹å§‹", "å·¦é‚Š", "å³é‚Š"])
            elif "è§’åº¦" in last_reply or "å·¦" in last_reply or "å³" in last_reply:
                predictions.extend(["é€Ÿåº¦", "é–‹å§‹", "æé«˜", "é™ä½"])
        
        return predictions[:8]  # å¢åŠ é æ¸¬æ•¸é‡
    
    def get_common_reply(self, query: str) -> Optional[str]:
        """ç²å–å¸¸ç”¨å›è¦†æ¨¡æ¿"""
        query_lower = _normalize_zh(query)
        
        # ç›´æ¥åŒ¹é…
        for key, replies in self.common_templates.items():
            if key in query_lower:
                import random
                return random.choice(replies)
        
        # æ¨¡ç³ŠåŒ¹é…
        for key, replies in self.common_templates.items():
            if fuzz.partial_ratio(query_lower, key) >= 80:
                import random
                return random.choice(replies)
        
        return None
    
    def _load_persistent_cache(self):
        """è¼‰å…¥æŒä¹…åŒ–å¿«å–"""
        if not app_config.preload.persistent_cache:
            return
        
        cache_file = app_config.preload.cache_file
        try:
            # ç¢ºä¿å¿«å–ç›®éŒ„å­˜åœ¨
            cache_dir = os.path.dirname(cache_file)
            if cache_dir and not os.path.exists(cache_dir):
                os.makedirs(cache_dir, exist_ok=True)
            
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è¼‰å…¥å¿«å–è³‡æ–™
                for query_hash, cache_data in data.get("cache", {}).items():
                    # æª¢æŸ¥å¿«å–æ˜¯å¦éæœŸ
                    if time.time() - cache_data["timestamp"] < app_config.preload.cache_ttl:
                        self.cache[query_hash] = cache_data
                
                print(f"ğŸ“‚ è¼‰å…¥æŒä¹…åŒ–å¿«å–ï¼š{len(self.cache)} å€‹é …ç›®")
            else:
                print("ğŸ“‚ æœªæ‰¾åˆ°å¿«å–æª”æ¡ˆï¼Œå°‡å»ºç«‹æ–°çš„å¿«å–")
                
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥å¿«å–å¤±æ•—ï¼š{e}")
    
    def _save_persistent_cache(self):
        """å„²å­˜æŒä¹…åŒ–å¿«å–"""
        if not app_config.preload.persistent_cache:
            return
        
        cache_file = app_config.preload.cache_file
        try:
            # ç¢ºä¿å¿«å–ç›®éŒ„å­˜åœ¨
            cache_dir = os.path.dirname(cache_file)
            if cache_dir and not os.path.exists(cache_dir):
                os.makedirs(cache_dir, exist_ok=True)
            
            # æº–å‚™å„²å­˜è³‡æ–™
            data = {
                "cache": self.cache,
                "metadata": {
                    "saved_at": time.time(),
                    "version": "1.0",
                    "total_items": len(self.cache)
                }
            }
            
            # å„²å­˜åˆ°æª”æ¡ˆ
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            self.last_save_time = time.time()
            print(f"ğŸ’¾ å¿«å–å·²å„²å­˜ï¼š{len(self.cache)} å€‹é …ç›®")
            
        except Exception as e:
            print(f"âš ï¸ å„²å­˜å¿«å–å¤±æ•—ï¼š{e}")
    
    def _should_auto_save(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²è‡ªå‹•å„²å­˜"""
        return (time.time() - self.last_save_time) >= app_config.preload.auto_save_interval
    
    def cache_reply(self, query: str, reply: str):
        """å¿«å–å›è¦†ï¼ˆæ”¯æ´è‡ªå‹•å„²å­˜ï¼‰"""
        if not app_config.preload.enabled:
            return
        
        query_hash = self._hash_query(query)
        
        # æª¢æŸ¥å¿«å–å¤§å°é™åˆ¶
        if len(self.cache) >= app_config.preload.max_cache_size:
            self._cleanup_cache()
        
        self.cache[query_hash] = {
            "reply": reply,
            "timestamp": time.time(),
            "count": 1
        }
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦è‡ªå‹•å„²å­˜
        if self._should_auto_save():
            self._save_persistent_cache()
    
    def get_cache_stats(self) -> dict:
        """ç²å–å¿«å–çµ±è¨ˆè³‡è¨Š"""
        return {
            "cache_size": len(self.cache),
            "rule_cache_size": len(self.rule_cache),
            "common_templates": len(self.common_templates),
            "prediction_queue": len(self.prediction_queue),
            "total_queries": sum(item["count"] for item in self.cache.values()),
            "total_rule_hits": sum(item["count"] for item in self.rule_cache.values()),
            "persistent_cache": app_config.preload.persistent_cache,
            "cache_file": app_config.preload.cache_file,
            "last_save": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(self.last_save_time))
        }
    
    def save_cache_now(self):
        """ç«‹å³å„²å­˜å¿«å–"""
        self._save_persistent_cache()
    
    def clear_cache(self):
        """æ¸…ç©ºå¿«å–"""
        self.cache.clear()
        self.rule_cache.clear()
        if app_config.preload.persistent_cache:
            self._save_persistent_cache()
        print("ğŸ—‘ï¸ å¿«å–å·²æ¸…ç©º")

# å…¨åŸŸå¿«å–å¯¦ä¾‹
reply_cache = ReplyTemplateCache()

# === æ¨¡å¼ç®¡ç†ç³»çµ± ===
class ModeManager:
    """æ¨¡å¼ç®¡ç†å™¨ï¼šè™•ç†æ§åˆ¶æ¨¡å¼å’Œæ€è€ƒæ¨¡å¼çš„åˆ‡æ›"""
    
    def __init__(self, default_mode: str = "control", think_on: str = "å•Ÿå‹•æ€è€ƒæ¨¡å¼", 
                 control_on: str = "å•Ÿå‹•æ§åˆ¶æ¨¡å¼", mismatch_reply: str = "æˆ‘ç¾åœ¨åœ¨æ§åˆ¶æ¨¡å¼ï¼Œè«‹ç”¨æ˜ç¢ºçš„æŒ‡ä»¤å†èªªä¸€æ¬¡ã€‚"):
        self.current_mode = default_mode  # "control" æˆ– "think"
        self.think_on_keyword = think_on
        self.control_on_keyword = control_on
        self.mismatch_reply = mismatch_reply
        self.mode_history = []  # è¨˜éŒ„æ¨¡å¼åˆ‡æ›æ­·å²
        
    def get_current_mode(self) -> str:
        """ç²å–ç•¶å‰æ¨¡å¼"""
        return self.current_mode
    
    def is_control_mode(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæ§åˆ¶æ¨¡å¼"""
        return self.current_mode == "control"
    
    def is_think_mode(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæ€è€ƒæ¨¡å¼"""
        return self.current_mode == "think"
    
    def check_mode_switch(self, text: str) -> Optional[str]:
        """æª¢æŸ¥æ˜¯å¦è§¸ç™¼æ¨¡å¼åˆ‡æ›ï¼Œè¿”å›åˆ‡æ›å¾Œçš„å›è¦†æˆ– None"""
        normalized_text = _normalize_zh(text)
        think_keyword = _normalize_zh(self.think_on_keyword)
        control_keyword = _normalize_zh(self.control_on_keyword)
        
        # æª¢æŸ¥æ˜¯å¦è¦åˆ‡æ›åˆ°æ€è€ƒæ¨¡å¼
        if think_keyword in normalized_text and self.current_mode == "control":
            self._switch_to_think()
            return f"å·²åˆ‡æ›åˆ°æ€è€ƒæ¨¡å¼ï¼Œç¾åœ¨å¯ä»¥ä½¿ç”¨ LLM é€²è¡Œå°è©±ã€‚"
        
        # æª¢æŸ¥æ˜¯å¦è¦åˆ‡æ›åˆ°æ§åˆ¶æ¨¡å¼
        if control_keyword in normalized_text and self.current_mode == "think":
            self._switch_to_control()
            return f"å·²åˆ‡æ›åˆ°æ§åˆ¶æ¨¡å¼ï¼Œåªä½¿ç”¨è¦å‰‡åŒ¹é…ï¼Œä¸ä½¿ç”¨ LLMã€‚"
        
        return None
    
    def _switch_to_think(self):
        """åˆ‡æ›åˆ°æ€è€ƒæ¨¡å¼"""
        old_mode = self.current_mode
        self.current_mode = "think"
        self.mode_history.append({
            "from": old_mode,
            "to": "think",
            "timestamp": time.time()
        })
        print(f"ğŸ”„ æ¨¡å¼åˆ‡æ›ï¼š{old_mode} â†’ think")
    
    def _switch_to_control(self):
        """åˆ‡æ›åˆ°æ§åˆ¶æ¨¡å¼"""
        old_mode = self.current_mode
        self.current_mode = "control"
        self.mode_history.append({
            "from": old_mode,
            "to": "control",
            "timestamp": time.time()
        })
        print(f"ğŸ”„ æ¨¡å¼åˆ‡æ›ï¼š{old_mode} â†’ control")
    
    def get_mismatch_reply(self) -> str:
        """ç²å–æ§åˆ¶æ¨¡å¼ä¸‹è¦å‰‡ä¸åŒ¹é…æ™‚çš„å›è¦†"""
        return self.mismatch_reply
    
    def get_mode_status(self) -> dict:
        """ç²å–æ¨¡å¼ç‹€æ…‹è³‡è¨Š"""
        return {
            "current_mode": self.current_mode,
            "think_keyword": self.think_on_keyword,
            "control_keyword": self.control_on_keyword,
            "switch_count": len(self.mode_history),
            "last_switch": self.mode_history[-1] if self.mode_history else None
        }

class PreloadManager:
    """é è¼‰å…¥ç®¡ç†å™¨"""
    
    def __init__(self, client: OpenAI):
        self.client = client
        self.preload_thread = None
        self.is_running = False
        self.preload_queue = []
    
    def start_background_preload(self):
        """å•Ÿå‹•èƒŒæ™¯é è¼‰å…¥åŸ·è¡Œç·’"""
        if not app_config.preload.enabled:
            return
        
        self.is_running = True
        self.preload_thread = threading.Thread(target=self._background_preload_worker, daemon=True)
        self.preload_thread.start()
        print("ğŸ”„ èƒŒæ™¯é è¼‰å…¥åŸ·è¡Œç·’å·²å•Ÿå‹•")
    
    def stop_background_preload(self):
        """åœæ­¢èƒŒæ™¯é è¼‰å…¥åŸ·è¡Œç·’"""
        self.is_running = False
        if self.preload_thread:
            self.preload_thread.join(timeout=1)
        print("â¹ï¸ èƒŒæ™¯é è¼‰å…¥åŸ·è¡Œç·’å·²åœæ­¢")
    
    def _background_preload_worker(self):
        """èƒŒæ™¯é è¼‰å…¥å·¥ä½œåŸ·è¡Œç·’"""
        while self.is_running:
            try:
                # è™•ç†é è¼‰å…¥ä½‡åˆ—
                if reply_cache.prediction_queue:
                    prediction = reply_cache.prediction_queue.pop(0)
                    self._preload_reply(prediction)
                
                # è™•ç†é è¼‰å…¥ä½‡åˆ—
                if self.preload_queue:
                    query = self.preload_queue.pop(0)
                    self._preload_reply(query)
                
                time.sleep(0.1)  # çŸ­æš«ä¼‘æ¯
                
            except Exception as e:
                print(f"âš ï¸ é è¼‰å…¥åŸ·è¡Œç·’éŒ¯èª¤ï¼š{e}")
                time.sleep(1)
    
    def _preload_reply(self, query: str):
        """é è¼‰å…¥å›è¦†"""
        try:
            # æª¢æŸ¥æ˜¯å¦å·²ç¶“å¿«å–
            if reply_cache.get_cached_reply(query):
                return
            
            # é¦–å…ˆæª¢æŸ¥è¦å‰‡åŒ¹é…
            rule_result = self._preload_rule_match(query)
            if rule_result:
                return
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å¸¸ç”¨å›è¦†æ¨¡æ¿
            common_reply = reply_cache.get_common_reply(query)
            if common_reply:
                reply_cache.cache_reply(query, common_reply)
                return
            
            # ä½¿ç”¨ LLM ç”Ÿæˆå›è¦†ï¼ˆä½å„ªå…ˆç´šï¼‰
            if len(reply_cache.cache) < app_config.preload.max_cache_size // 2:
                reply = self._generate_preload_reply(query)
                if reply:
                    reply_cache.cache_reply(query, reply)
        
        except Exception as e:
            print(f"âš ï¸ é è¼‰å…¥å›è¦†å¤±æ•—ï¼š{e}")
    
    def _preload_rule_match(self, query: str) -> bool:
        """é è¼‰å…¥è¦å‰‡åŒ¹é…çµæœ"""
        if not app_config.preload.preload_rules:
            return False
            
        try:
            # ä½¿ç”¨é è¨­è¦å‰‡æª”æ¡ˆé€²è¡ŒåŒ¹é…
            default_rules_path = "rules/badminton_rules.yaml"
            if os.path.exists(default_rules_path):
                matcher = RuleMatcher(default_rules_path)
                hit = matcher.match(query)
                if hit:
                    # å¿«å–è¦å‰‡åŒ¹é…çµæœ
                    reply_cache.cache_rule_result(query, hit)
                    
                    # ç”Ÿæˆä¸¦å¿«å–å›è¦†
                    context = {"balls_left": 48}
                    reply_text = format_reply(hit.get("reply", {}).get("text", ""), context)
                    reply_cache.cache_reply(query, reply_text)
                    return True
        except Exception as e:
            print(f"âš ï¸ é è¼‰å…¥è¦å‰‡åŒ¹é…å¤±æ•—ï¼š{e}")
        
        return False
    
    def _generate_preload_reply(self, query: str) -> Optional[str]:
        """ç”Ÿæˆé è¼‰å…¥å›è¦†"""
        try:
            # ä½¿ç”¨ç°¡æ½”çš„ç³»çµ±æç¤º
            system_prompt = "ä½ æ˜¯ç¾½çƒç™¼çƒæ©ŸåŠ©ç†ï¼Œè«‹ç”¨ç°¡æ½”çš„1-2å¥è©±å›è¦†ã€‚"
            
            reply = llm_reply(
                self.client,
                query,
                system_prompt,
                temperature=0.3,  # è¼ƒä½æº«åº¦ç¢ºä¿ä¸€è‡´æ€§
                max_tokens=50,    # é™åˆ¶é•·åº¦
                conversation_history=None
            )
            return reply
        
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆé è¼‰å…¥å›è¦†å¤±æ•—ï¼š{e}")
            return None
    
    def add_to_preload_queue(self, query: str):
        """æ·»åŠ æŸ¥è©¢åˆ°é è¼‰å…¥ä½‡åˆ—"""
        if query not in self.preload_queue:
            self.preload_queue.append(query)
    
    def preload_common_queries(self):
        """é è¼‰å…¥å¸¸ç”¨æŸ¥è©¢ï¼ˆåŸºæ–¼è¦å‰‡ç³»çµ±ï¼‰"""
        common_queries = [
            # åŸºæœ¬æ§åˆ¶
            "é–‹å§‹ç™¼çƒ", "åœæ­¢ç™¼çƒ", "é–‹å§‹è¨“ç·´", "åœæ­¢è¨“ç·´",
            # é€Ÿåº¦æ§åˆ¶
            "å¿«é€Ÿ", "æ…¢é€Ÿ", "ä¸­é€Ÿ", "åŠ é€Ÿ", "æ¸›é€Ÿ",
            # è§’åº¦æ§åˆ¶
            "å·¦é‚Š", "å³é‚Š", "ä¸­é–“", "å·¦è½‰", "å³è½‰",
            # é«˜åº¦æ§åˆ¶
            "æé«˜", "é™ä½", "ä¸Šå‡", "ä¸‹é™",
            # è¨“ç·´æ¨¡å¼
            "å‰å ´ç·´ç¿’", "å¾Œå ´ç·´ç¿’", "æ®ºçƒç·´ç¿’", "åŠçƒç·´ç¿’",
            "æ­£æ‰‹ç·´ç¿’", "åæ‰‹ç·´ç¿’", "ç›´ç·šçƒ", "æ–œç·šçƒ",
            # ç‹€æ…‹æŸ¥è©¢
            "å‰©é¤˜çƒæ•¸", "ç¾åœ¨ç‹€æ…‹", "é€²åº¦å¦‚ä½•", "å ±å‘Šç‹€æ…‹",
            # åŸºæœ¬äº’å‹•
            "å¹«åŠ©", "è¬è¬", "ä½ å¥½", "å†è¦‹", "è¾›è‹¦äº†"
        ]
        
        for query in common_queries:
            self.add_to_preload_queue(query)
        
        print(f"ğŸ“‹ å·²æ’ç¨‹ {len(common_queries)} å€‹å¸¸ç”¨æŸ¥è©¢é è¼‰å…¥")

class RuleMatcher:
    """å„ªåŒ–çš„è¦å‰‡åŒ¹é…å™¨ï¼Œæ”¯æ´é ç·¨è­¯å’Œå¿«å–"""
    
    def __init__(self, rules_path: str):
        self.rules_path = rules_path
        self._compiled_regex = {}
        self._rules_data = None
        self._last_mtime = 0.0
        self._cache_enabled = True
    
    def _load_rules(self) -> dict:
        """è¼‰å…¥è¦å‰‡ä¸¦é ç·¨è­¯æ­£å‰‡è¡¨é”å¼"""
        global _RULES_CACHE
        p = os.path.abspath(self.rules_path)
        mtime = os.path.getmtime(p)
        
        # æª¢æŸ¥å¿«å–
        if (_RULES_CACHE["path"] == p and 
            _RULES_CACHE["mtime"] == mtime and 
            _RULES_CACHE["data"] is not None):
            return _RULES_CACHE["data"]
        
        with open(p, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
        
        # é è™•ç†å’Œé ç·¨è­¯
        for r in data.get("rules", []):
            r.setdefault("priority", 0)
            r.setdefault("match", {})
            r["match"].setdefault("contains", [])
            r["match"].setdefault("regex", [])
            r["match"].setdefault("fuzzy", [])
            
            # é å»ºæ­£è¦åŒ–å­—ä¸²ä»¥åŠ é€Ÿ
            r["_contains_norm"] = [_normalize_zh(x) for x in r["match"]["contains"]]
            
            # é ç·¨è­¯æ­£å‰‡è¡¨é”å¼
            r["_compiled_regex"] = self._compile_regex(r["match"]["regex"])
        
        # æ›´æ–°å¿«å–
        _RULES_CACHE = {
            "path": p, 
            "mtime": mtime, 
            "data": data,
            "compiled_regex": {}
        }
        
        return data
    
    def _compile_regex(self, patterns: list) -> list:
        """é ç·¨è­¯æ­£å‰‡è¡¨é”å¼"""
        compiled = []
        for pattern in patterns:
            try:
                compiled.append(re.compile(pattern))
            except re.error as e:
                print(f"âš ï¸ ç„¡æ•ˆçš„æ­£å‰‡è¡¨é”å¼ï¼š{pattern} - {e}")
        return compiled
    
    def match(self, text: str) -> Optional[dict]:
        """åŒ¹é…è¦å‰‡ï¼ˆæ”¯æ´å¿«å–ï¼‰"""
        if not text.strip():
            return None
        
        # é¦–å…ˆæª¢æŸ¥å¿«å–
        if self._cache_enabled:
            cached_result = reply_cache.get_cached_rule_result(text)
            if cached_result:
                print("âš¡ ä½¿ç”¨è¦å‰‡å¿«å–çµæœ")
                return cached_result
        
        rules_data = self._load_rules()
        ntext = _normalize_zh(text)
        rules = sorted(rules_data.get("rules", []), 
                      key=lambda r: r.get("priority", 0), reverse=True)
        fuzzy_th = rules_data.get("globals", {}).get("fuzzy_threshold", 86)

        for r in rules:
            # 1) åŒ…å«å¼ï¼ˆæ­£è¦åŒ–ï¼‰
            for key_norm in r.get("_contains_norm", []):
                if key_norm and key_norm in ntext:
                    # å¿«å–çµæœ
                    if self._cache_enabled:
                        reply_cache.cache_rule_result(text, r)
                    return r
            
            # 2) æ­£å‰‡ï¼ˆä½¿ç”¨é ç·¨è­¯ï¼‰
            for compiled_regex in r.get("_compiled_regex", []):
                if compiled_regex.search(text):
                    # å¿«å–çµæœ
                    if self._cache_enabled:
                        reply_cache.cache_rule_result(text, r)
                    return r
            
            # 3) æ¨¡ç³Šæ¯”å°ï¼ˆå°æ­£è¦åŒ–å¾Œå­—ä¸²ï¼‰
            for k in r["match"].get("fuzzy", []):
                if fuzz.partial_ratio(ntext, _normalize_zh(k)) >= fuzzy_th:
                    # å¿«å–çµæœ
                    if self._cache_enabled:
                        reply_cache.cache_rule_result(text, r)
                    return r
        
        return None

# === Wake word config ===
DEFAULT_WAKE = "å•Ÿå‹•èªéŸ³ç™¼çƒæ©Ÿ"
DEFAULT_WAKE_REPLY = "å½¥æ¾¤æ‚¨å¥½ï¼Œæˆ‘æ˜¯ä½ çš„æ™ºæ…§ç¾½çƒç™¼çƒæ©ŸåŠ©ç†ï¼Œä»Šå¤©æƒ³ç·´ä»€éº¼å‘¢ï¼Ÿ"

_ZH_PUNCT = "ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼šã€Œã€ã€ã€ï¼ˆï¼‰ã€ã€‘ã€Šã€‹â€”ï¼â€¦â€§,.!?;:()[]{}<>~`@#$%^&*-_=+|/\\\"'\u3000 "  # å«å…¨å½¢ç©ºç™½

def _normalize_zh(s: str) -> str:
    s = (s or "").strip().lower()
    for ch in _ZH_PUNCT:
        s = s.replace(ch, "")
    return s

def is_wake_hit(text: str, wake: str) -> bool:
    """ç§»é™¤ç©ºç™½/æ¨™é»ï¼Œæ¯”å°æ˜¯å¦åŒ…å«å–šé†’è©ï¼ˆå®¹å¿æœ‰ç©ºæ ¼æˆ–æ¨™é»ï¼‰ã€‚"""
    return _normalize_zh(wake) in _normalize_zh(text)


def load_rules(path: str) -> dict:
    """å«ç°¡æ˜“å¿«å–ï¼›è‹¥è¦å‰‡æª” mtime è®Šå‹•æ‰é‡è®€ã€‚"""
    global _RULES_CACHE
    p = os.path.abspath(path)
    mtime = os.path.getmtime(p)
    if _RULES_CACHE["path"] == p and _RULES_CACHE["mtime"] == mtime and _RULES_CACHE["data"] is not None:
        return _RULES_CACHE["data"]

    with open(p, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}
    # é è™•ç†ï¼špriority é è¨­ã€æ¬„ä½å®¹éŒ¯
    for r in data.get("rules", []):
        r.setdefault("priority", 0)
        r.setdefault("match", {})
        r["match"].setdefault("contains", [])
        r["match"].setdefault("regex", [])
        r["match"].setdefault("fuzzy", [])
        # é å»ºæ­£è¦åŒ–å­—ä¸²ä»¥åŠ é€Ÿ
        r["_contains_norm"] = [_normalize_zh(x) for x in r["match"]["contains"]]
    _RULES_CACHE = {"path": p, "mtime": mtime, "data": data}
    return data

def match_rules(text: str, rules_data: dict) -> Optional[dict]:
    """å›å‚³ç¬¬ä¸€å€‹å‘½ä¸­çš„è¦å‰‡ï¼ˆä¾ priority ç”±é«˜åˆ°ä½ï¼‰ã€‚"""
    if not text.strip():
        return None
    ntext = _normalize_zh(text)
    rules = sorted(rules_data.get("rules", []), key=lambda r: r.get("priority", 0), reverse=True)
    fuzzy_th = rules_data.get("globals", {}).get("fuzzy_threshold", 86)

    for r in rules:
        # 1) åŒ…å«å¼ï¼ˆæ­£è¦åŒ–ï¼‰
        for key_norm in r.get("_contains_norm", []):
            if key_norm and key_norm in ntext:
                return r
        # 2) æ­£å‰‡ï¼ˆç”¨åŸæ–‡ï¼‰
        for pat in r["match"].get("regex", []):
            try:
                if re.search(pat, text):
                    return r
            except re.error:
                continue
        # 3) æ¨¡ç³Šæ¯”å°ï¼ˆå°æ­£è¦åŒ–å¾Œå­—ä¸²ï¼‰
        for k in r["match"].get("fuzzy", []):
            if fuzz.partial_ratio(ntext, _normalize_zh(k)) >= fuzzy_th:
                return r
    return None

def format_reply(template: str, context: dict) -> str:
    try:
        return template.format(**context)
    except Exception:
        return template  # ç¼ºå°‘è®Šæ•¸å°±ç”¨åŸæ¨£


def list_avfoundation_devices() -> None:
    """åˆ—å‡º avfoundation è¼¸å…¥è£ç½®ï¼ˆmacOSï¼‰ã€‚"""
    if shutil.which("ffmpeg") is None:
        print("æ‰¾ä¸åˆ° ffmpegï¼Œè«‹å…ˆå®‰è£ï¼šbrew install ffmpeg")
        return
    # åˆ—å‡ºè£ç½®
    subprocess.run(["ffmpeg", "-f", "avfoundation", "-list_devices", "true", "-i", ""], check=False)


def list_sd_input_devices() -> None:
    """åˆ—å‡º sounddevice å¯ç”¨è¼¸å…¥è£ç½®ã€‚"""
    import sounddevice as sd  # å»¶å¾ŒåŒ¯å…¥ï¼Œé¿å…æœªå®‰è£æ™‚å½±éŸ¿å…¶å®ƒåŠŸèƒ½
    print("=== å¯ç”¨éŒ„éŸ³è£ç½®ï¼ˆsounddevice input devicesï¼‰===")
    for i, d in enumerate(sd.query_devices()):
        if d.get("max_input_channels", 0) > 0:
            mark = " (é è¨­)" if sd.default.device and i == sd.default.device[0] else ""
            print(f"[{i}] {d.get('name','?')}  max_in={d.get('max_input_channels',0)}{mark}")


def record_wav_ffmpeg(seconds: int, out_path: str, adev: int = 0) -> None:
    """ä»¥ ffmpegï¼ˆavfoundationï¼‰åœ¨ macOS éŒ„éŸ³ä½œç‚ºå‚™æ´ã€‚
    éœ€å®‰è£ ffmpegï¼šbrew install ffmpeg
    """
    if shutil.which("ffmpeg") is None:
        raise RuntimeError("æ‰¾ä¸åˆ° ffmpegï¼Œè«‹å…ˆå®‰è£ï¼šbrew install ffmpeg")
    # ä½¿ç”¨é è¨­éº¥å…‹é¢¨ï¼ˆ:0ï¼‰ï¼Œ16k/å–®è²é“/16-bit PCM
    cmd = [
        "ffmpeg", "-y",
        "-f", "avfoundation", "-i", f":{adev}",
        "-t", str(seconds),
        "-ac", str(CHANNELS),
        "-ar", str(SAMPLE_RATE),
        "-acodec", "pcm_s16le",
        out_path,
    ]
    print(f"ğŸ™ï¸ ffmpeg å‚™æ´éŒ„éŸ³ {seconds} ç§’ï¼ˆadev={adev}ï¼‰...")
    subprocess.run(cmd, check=True)
    print(f"âœ… å·²å„²å­˜ï¼ˆffmpegï¼‰ï¼š{out_path}")


def record_wav(seconds: int, out_path: str, adev: int = 0, sd_device: Optional[int] = None) -> None:
    print(f"ğŸ™ï¸ é–‹å§‹éŒ„éŸ³ {seconds} ç§’...")
    try:
        # å„ªå…ˆå˜—è©¦ sounddevice éŒ„éŸ³
        if sd_device is not None:
            sd.default.device = (sd_device, None)  # (input, output)
        sd.default.samplerate = SAMPLE_RATE
        sd.default.channels = CHANNELS
        audio = sd.rec(int(seconds * SAMPLE_RATE), dtype="int16")
        sd.wait()
        wavwrite(out_path, SAMPLE_RATE, audio)
        print(f"âœ… å·²å„²å­˜ï¼š{out_path}")
    except MemoryError as e:
        print("âš ï¸ sounddevice/cffi è¨˜æ†¶é«”é™åˆ¶ï¼Œæ”¹ç”¨ ffmpeg å‚™æ´ã€‚", e)
        record_wav_ffmpeg(seconds, out_path, adev=adev)
    except Exception as e:
        print("âš ï¸ sounddevice éŒ„éŸ³å¤±æ•—ï¼Œæ”¹ç”¨ ffmpeg å‚™æ´ã€‚", e)
        record_wav_ffmpeg(seconds, out_path, adev=adev)


def _record_with_vad_common(out_path: str, aggressiveness: Optional[int] = None, 
                           silence_ms: Optional[int] = None, sd_device: Optional[int] = None, 
                           realtime_transcribe: bool = False, low_latency: bool = False) -> str:
    """VAD éŒ„éŸ³çš„å…±é€šé‚è¼¯"""
    if not WEBRTCVAD_AVAILABLE:
        raise RuntimeError("webrtcvad æœªå®‰è£ï¼Œè«‹åŸ·è¡Œï¼špip install webrtcvad")
    
    if realtime_transcribe:
        print("âš ï¸ ä¸æ”¯æ´å³æ™‚è½‰éŒ„ï¼Œå°‡åœ¨éŒ„éŸ³å®Œæˆå¾Œé€²è¡Œè½‰éŒ„")
        realtime_transcribe = False
    
    # ä½¿ç”¨é…ç½®ä¸­çš„é è¨­å€¼ï¼Œæ”¯æ´ä½å»¶é²æ¨¡å¼
    if aggressiveness is None:
        aggressiveness = app_config.audio.aggressiveness
    if silence_ms is None:
        if low_latency:
            silence_ms = app_config.audio.ultra_fast_silence_ms
        else:
            silence_ms = app_config.audio.silence_ms
    
    print(f"ğŸ™ï¸ é–‹å§‹éŒ„éŸ³ï¼ˆVAD æ¨¡å¼ï¼‰ï¼Œåœé “ >{silence_ms/1000:.1f} ç§’è‡ªå‹•çµæŸâ€¦")
    
    # è¨­å®š VAD
    vad = webrtcvad.Vad(aggressiveness)
    
    # éŒ„éŸ³åƒæ•¸ï¼ˆä½¿ç”¨é…ç½®ï¼‰
    frame_duration_ms = app_config.audio.frame_duration_ms
    frame_size = int(app_config.audio.sample_rate * frame_duration_ms / 1000)
    silence_frames = int(silence_ms / frame_duration_ms)
    
    # è¨­å®š sounddevice
    if sd_device is not None:
        sd.default.device = (sd_device, None)
    sd.default.samplerate = app_config.audio.sample_rate
    sd.default.channels = app_config.audio.channels
    
    # é–‹å§‹éŒ„éŸ³
    audio_buffer = []
    consecutive_silence = 0
    has_speech = False
    speech_frames = 0
    # ä½å»¶é²æ¨¡å¼ä½¿ç”¨æ›´å°‘çš„èªéŸ³å¹€è¦æ±‚
    min_speech_frames = app_config.audio.min_speech_frames_fast if low_latency else app_config.audio.min_speech_frames
    
    # _log_memory_usage("éŒ„éŸ³é–‹å§‹å‰")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„
    
    try:
        with sd.InputStream(samplerate=app_config.audio.sample_rate, 
                          channels=app_config.audio.channels, 
                          dtype="int16", blocksize=frame_size) as stream:
            while True:
                # è®€å–ä¸€å¹€éŸ³è¨Š
                audio_frame, overflowed = stream.read(frame_size)
                if overflowed:
                    print("âš ï¸ éŸ³è¨Šç·©è¡å€æº¢å‡º")
                
                # è½‰æ›ç‚º bytes ä¾› VAD ä½¿ç”¨
                frame_bytes = audio_frame.tobytes()
                
                # VAD åµæ¸¬
                is_speech = vad.is_speech(frame_bytes, app_config.audio.sample_rate)
                
                if is_speech:
                    consecutive_silence = 0
                    has_speech = True
                    speech_frames += 1
                else:
                    consecutive_silence += 1
                
                # å„²å­˜éŸ³è¨Šå¹€
                audio_buffer.append(audio_frame)
                
                # è¨˜æ†¶é«”å„ªåŒ–ï¼šå®šæœŸæ¸…ç†èˆŠå¹€
                if len(audio_buffer) % 100 == 0:  # æ¯ 100 å¹€æª¢æŸ¥ä¸€æ¬¡
                    audio_buffer = _cleanup_old_frames(audio_buffer)
                
                # æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢
                if has_speech and speech_frames >= min_speech_frames and consecutive_silence >= silence_frames:
                    print("\nğŸ”‡ åµæ¸¬åˆ°éœéŸ³ï¼Œåœæ­¢éŒ„éŸ³")
                    break
                
                # é˜²æ­¢éŒ„éŸ³éé•·
                if len(audio_buffer) * frame_duration_ms > app_config.audio.max_recording_ms:
                    print("\nâ° éŒ„éŸ³æ™‚é–“éé•·ï¼Œè‡ªå‹•åœæ­¢")
                    break
    
    except Exception as e:
        print(f"\nâš ï¸ VAD éŒ„éŸ³å¤±æ•—ï¼š{e}")
        raise
    
    # åˆä½µéŸ³è¨Šä¸¦å„²å­˜
    if audio_buffer:
        # æœ€çµ‚è¨˜æ†¶é«”å„ªåŒ–
        audio_buffer = _optimize_audio_buffer(audio_buffer)
        # _log_memory_usage("éŸ³è¨Šè™•ç†å‰")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„
        
        full_audio = np.concatenate(audio_buffer, axis=0)
        wavwrite(out_path, app_config.audio.sample_rate, full_audio)
        
        # æ¸…ç†è¨˜æ†¶é«”
        del audio_buffer
        del full_audio
        
        # _log_memory_usage("éŸ³è¨Šè™•ç†å¾Œ")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„
        print(f"âœ… å·²å­˜æª” {out_path}")
    else:
        print("âš ï¸ æ²’æœ‰éŒ„åˆ°ä»»ä½•éŸ³è¨Š")
        # å»ºç«‹ä¸€å€‹ç©ºçš„ wav æª”æ¡ˆ
        empty_audio = np.zeros(int(app_config.audio.sample_rate * 0.1), dtype="int16")
        wavwrite(out_path, app_config.audio.sample_rate, empty_audio)
        del empty_audio
    
    return ""  # ä¸æ”¯æ´å³æ™‚è½‰éŒ„ï¼Œè¿”å›ç©ºå­—ä¸²


def record_until_silence_realtime(out_path: str = "input.wav", aggressiveness: int = 2, silence_ms: int = 300, sd_device: Optional[int] = None, realtime_transcribe: bool = False) -> str:
    """ä½¿ç”¨ webrtcvad åµæ¸¬èªéŸ³æ´»å‹•ï¼Œè‡ªå‹•åœæ­¢éŒ„éŸ³ï¼Œå¯é¸å³æ™‚è½‰éŒ„"""
    return _record_with_vad_common(out_path, aggressiveness, silence_ms, sd_device, realtime_transcribe)


def record_until_silence(out_path: str = "input.wav", aggressiveness: int = 2, silence_ms: int = 300, sd_device: Optional[int] = None, low_latency: bool = False) -> None:
    """ä½¿ç”¨ webrtcvad åµæ¸¬èªéŸ³æ´»å‹•ï¼Œè‡ªå‹•åœæ­¢éŒ„éŸ³"""
    _record_with_vad_common(out_path, aggressiveness, silence_ms, sd_device, low_latency=low_latency)


def s2twp(text: str, enabled: bool = True) -> str:
    if not enabled or not text:
        return text
    if _cc is None:
        print("â„¹ï¸ å»ºè­°å®‰è£ opencc-python-reimplemented ä»¥è¼¸å‡ºç¹é«”ï¼špip install opencc-python-reimplemented")
        return text
    return _cc.convert(text)


def asr_transcribe_whisper(client: OpenAI, wav_path: str, language: str = "zh", model: str = "whisper-1") -> str:
    """ä½¿ç”¨ Whisper API é€²è¡ŒèªéŸ³è­˜åˆ¥"""
    if not os.path.exists(wav_path):
        raise RuntimeError(f"éŸ³è¨Šæª”æ¡ˆä¸å­˜åœ¨ï¼š{wav_path}")
    
    print(f"ğŸ§  èªéŸ³è½‰éŒ„ä¸­... (èªè¨€: {language}, æ¨¡å‹: {model})")
    
    try:
        with open(wav_path, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model=model,
                file=audio_file,
                language=language,
                response_format="text"
            )
        
        # æ¸…ç†è½‰éŒ„çµæœ
        text = transcript.strip() if isinstance(transcript, str) else str(transcript).strip()
        return text
        
    except Exception as e:
        print(f"âŒ èªéŸ³è½‰éŒ„å¤±æ•—ï¼š{e}")
        return ""


def asr_transcribe_whisper_with_retry(client: OpenAI, wav_path: str, language: str = "zh", 
                                    model: str = "whisper-1", max_retries: Optional[int] = None) -> str:
    """å¸¶é‡è©¦æ©Ÿåˆ¶çš„èªéŸ³è½‰éŒ„"""
    if max_retries is None:
        max_retries = app_config.max_retries
    
    for attempt in range(max_retries):
        try:
            result = asr_transcribe_whisper(client, wav_path, language, model)
            if result:  # å¦‚æœæˆåŠŸå–å¾—çµæœ
                return result
            elif attempt < max_retries - 1:  # å¦‚æœçµæœç‚ºç©ºä½†ä¸æ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦
                print(f"âš ï¸ è½‰éŒ„çµæœç‚ºç©ºï¼Œé‡è©¦ä¸­... ({attempt + 1}/{max_retries})")
                time.sleep(app_config.retry_delay)
        except Exception as e:
            if attempt == max_retries - 1:
                print(f"âŒ è½‰éŒ„æœ€çµ‚å¤±æ•—ï¼š{e}")
                raise
            print(f"âš ï¸ è½‰éŒ„å¤±æ•—ï¼Œé‡è©¦ä¸­... ({attempt + 1}/{max_retries}): {e}")
            time.sleep(app_config.retry_delay)
    
    return ""


def llm_reply_with_retry(client: OpenAI, user_text: str, system_prompt: Optional[str], 
                        *, temperature: float, max_tokens: int, 
                        conversation_history: Optional[list] = None,
                        max_retries: Optional[int] = None) -> str:
    """å¸¶é‡è©¦æ©Ÿåˆ¶çš„ LLM å›è¦†"""
    if max_retries is None:
        max_retries = app_config.max_retries
    
    for attempt in range(max_retries):
        try:
            return llm_reply(client, user_text, system_prompt, 
                           temperature=temperature, max_tokens=max_tokens,
                           conversation_history=conversation_history)
        except Exception as e:
            if attempt == max_retries - 1:
                print(f"âŒ LLM å›è¦†æœ€çµ‚å¤±æ•—ï¼š{e}")
                raise
            print(f"âš ï¸ LLM å›è¦†å¤±æ•—ï¼Œé‡è©¦ä¸­... ({attempt + 1}/{max_retries}): {e}")
            time.sleep(app_config.retry_delay)
    
    return ""


def tts_speak_with_retry(client: OpenAI, text: str, voice: str, output_path: str, 
                        speed_factor: float = 1.0, max_retries: Optional[int] = None) -> None:
    """å¸¶é‡è©¦æ©Ÿåˆ¶çš„ TTS èªéŸ³åˆæˆ"""
    if max_retries is None:
        max_retries = app_config.max_retries
    
    for attempt in range(max_retries):
        try:
            tts_speak(client, text, voice, output_path, speed_factor)
            return  # æˆåŠŸå‰‡ç›´æ¥è¿”å›
        except Exception as e:
            if attempt == max_retries - 1:
                print(f"âŒ TTS åˆæˆæœ€çµ‚å¤±æ•—ï¼š{e}")
                raise
            print(f"âš ï¸ TTS åˆæˆå¤±æ•—ï¼Œé‡è©¦ä¸­... ({attempt + 1}/{max_retries}): {e}")
            time.sleep(app_config.retry_delay)


def _parallel_api_calls(client: OpenAI, asr_text: str, args: argparse.Namespace, 
                       conversation_history: Optional[list]) -> tuple[str, str]:
    """ä¸¦è¡ŒåŸ·è¡Œ LLM å’Œ TTS èª¿ç”¨ä»¥é™ä½å»¶é²"""
    if not app_config.parallel_processing:
        # å¦‚æœä¸å•Ÿç”¨ä¸¦è¡Œè™•ç†ï¼Œä½¿ç”¨åŸæœ‰é †åº
        reply = llm_reply_with_retry(
            client, asr_text, args.system,
            temperature=args.temperature, max_tokens=args.max_tokens,
            conversation_history=conversation_history
        )
        return reply, ""
    
    # ä¸¦è¡Œè™•ç†ï¼šåŒæ™‚æº–å‚™ LLM å’Œé å…ˆè¼‰å…¥ TTS
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        # æäº¤ LLM ä»»å‹™
        llm_future = executor.submit(
            llm_reply_with_retry,
            client, asr_text, args.system,
            temperature=args.temperature, max_tokens=args.max_tokens,
            conversation_history=conversation_history
        )
        
        # ç­‰å¾… LLM å®Œæˆ
        reply = llm_future.result()
        
        # ç«‹å³è¿”å›çµæœï¼ŒTTS åœ¨å¾Œå°è™•ç†
        return reply, ""


def _streaming_tts(client: OpenAI, text: str, voice: str, output_path: str, 
                  speed_factor: float = 1.0) -> None:
    """æµå¼ TTS è™•ç†ï¼ˆåœ¨å¾Œå°åŸ·è¡Œï¼‰"""
    try:
        tts_speak_with_retry(client, text, voice, output_path, speed_factor)
        print(f"âœ… èƒŒæ™¯ TTS å®Œæˆï¼š{output_path}")
    except Exception as e:
        print(f"âŒ èƒŒæ™¯ TTS å¤±æ•—ï¼š{e}")


def llm_reply(client: OpenAI, user_text: str, system_prompt: Optional[str], *, temperature: float, max_tokens: int, conversation_history: Optional[list] = None) -> str:
    # ä»¥ç¹é«”å›è¦†ï¼Œä¿ç•™å£èªåŒ–èªæ°£ï¼›è‹¥æä¾› system_promptï¼ŒåŠ å…¥å°è©±é¢¨æ ¼æŒ‡ç¤º
    messages = []
    
    # æ·»åŠ ç³»çµ±æç¤º
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    # æ·»åŠ å°è©±æ­·å²ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    if conversation_history:
        messages.extend(conversation_history)
    
    # æ·»åŠ ç•¶å‰ç”¨æˆ¶è¼¸å…¥
    messages.append({"role": "user", "content": user_text})
    
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return resp.choices[0].message.content.strip()


def adjust_speed(input_path: str, output_path: str, speed_factor: float) -> None:
    """èª¿æ•´éŸ³è¨Šæ’­æ”¾é€Ÿåº¦"""
    if not PYDUB_AVAILABLE:
        print("âš ï¸ ç„¡æ³•èª¿æ•´èªé€Ÿï¼šæœªå®‰è£ pydub")
        return
    
    try:
        # è¼‰å…¥éŸ³è¨Šæª”æ¡ˆ
        audio = AudioSegment.from_mp3(input_path)
        
        # èª¿æ•´èªé€Ÿ
        if speed_factor != 1.0:
            # ä½¿ç”¨ pydub çš„ speedup æ•ˆæœ
            audio = speedup(audio, playback_speed=speed_factor)
        
        # å„²å­˜èª¿æ•´å¾Œçš„éŸ³è¨Š
        audio.export(output_path, format="mp3")
        print(f"âœ… å·²èª¿æ•´èªé€Ÿè‡³ {speed_factor}x å€é€Ÿ")
        
    except Exception as e:
        print(f"âŒ èªé€Ÿèª¿æ•´å¤±æ•—ï¼š{e}")


def tts_speak(client: OpenAI, text: str, voice: str, output_path: str, speed_factor: float = 1.0) -> None:
    if not text:
        raise ValueError("TTS è¼¸å…¥æ–‡å­—ç‚ºç©ºã€‚")
    
    # å¦‚æœä¸éœ€è¦èª¿æ•´èªé€Ÿï¼Œç›´æ¥è¼¸å‡º
    if speed_factor == 1.0:
        response = client.audio.speech.create(
            model="tts-1",
            voice=voice,
            input=text,
        )
        response.stream_to_file(output_path)
    else:
        # éœ€è¦èª¿æ•´èªé€Ÿæ™‚ï¼Œå…ˆç”¢ç”Ÿæš«å­˜æª”æ¡ˆ
        temp_output = f"temp_{output_path}"
        response = client.audio.speech.create(
            model="tts-1",
            voice=voice,
            input=text,
        )
        response.stream_to_file(temp_output)
        
        # èª¿æ•´èªé€Ÿ
        adjust_speed(temp_output, output_path, speed_factor)
        
        # åˆªé™¤æš«å­˜æª”æ¡ˆ
        if os.path.exists(temp_output):
            os.remove(temp_output)


def autoplay_mac(path: str, enabled: bool = True) -> None:
    if not enabled:
        return
    try:
        subprocess.run(["afplay", path], check=False)
    except Exception:
        pass


def _prepare_audio_input(args: argparse.Namespace) -> tuple[str, str]:
    """æº–å‚™éŸ³è¨Šè¼¸å…¥ï¼Œè¿”å› (wav_path, asr_text)"""
    wav_path = args.input
    asr_text = ""
    
    if not wav_path:
        tmp_path = pathlib.Path(args.tmp or "input.wav").as_posix()
        if args.realtime:
            # å³æ™‚è½‰éŒ„æ¨¡å¼ï¼šéŒ„éŸ³åŒæ™‚å³æ™‚è½‰éŒ„
            asr_text = record_until_silence_realtime(tmp_path, sd_device=args.sd_device, realtime_transcribe=True)
            wav_path = tmp_path
        elif args.duration is not None:
            # å‚³çµ±æ¨¡å¼ï¼šå›ºå®šç§’æ•¸éŒ„éŸ³
            record_wav(args.duration, tmp_path, adev=args.adev, sd_device=args.sd_device)
            wav_path = tmp_path
        else:
            # é è¨­ VAD æ¨¡å¼ï¼šè‡ªå‹•åµæ¸¬èªéŸ³çµæŸï¼ˆæ”¯æ´ä½å»¶é²ï¼‰
            low_latency = app_config.low_latency_mode
            record_until_silence(tmp_path, sd_device=args.sd_device, low_latency=low_latency)
            wav_path = tmp_path
    
    return wav_path, asr_text


def _process_asr(client: OpenAI, wav_path: str, asr_text: str, args: argparse.Namespace) -> str:
    """è™•ç†èªéŸ³è­˜åˆ¥"""
    # ASR (ä½¿ç”¨ Whisper API)
    if not args.realtime or not asr_text:
        show_progress_with_dots("ğŸ§  èªéŸ³è½‰éŒ„ä¸­", 3)
        asr_text = asr_transcribe_whisper_with_retry(client, wav_path, args.whisper_language, args.whisper_model)
    
    show_progress("ğŸ”„ è½‰æ›ç‚ºç¹é«”ä¸­æ–‡", 0.3)
    asr_text_trad = s2twp(asr_text, enabled=not args.no_s2twp)
    print("\n==== è½‰éŒ„çµæœ ====")
    print(asr_text_trad.strip())
    print("==================\n")
    
    return asr_text_trad


def _handle_rules_matching(asr_text_trad: str, args: argparse.Namespace, mode_manager: Optional['ModeManager'] = None) -> Optional[tuple[str, str]]:
    """è™•ç†è¦å‰‡åŒ¹é…ï¼Œè¿”å› (reply_text, voice) æˆ– None"""
    if not (args.rules and not args.no_rules and RULES_AVAILABLE):
        if args.rules and not RULES_AVAILABLE:
            print("âš ï¸ è¦å‰‡ç³»çµ±ä¾è³´æœªå®‰è£ï¼Œè«‹åŸ·è¡Œï¼špip install pyyaml rapidfuzz")
        return None
    
    if app_config.low_latency_mode:
        show_fast_progress("ğŸ” æª¢æŸ¥è¦å‰‡åŒ¹é…")
    else:
        show_progress("ğŸ” æª¢æŸ¥è¦å‰‡åŒ¹é…", 0.2)
    
    try:
        # ä½¿ç”¨å„ªåŒ–çš„ RuleMatcher
        matcher = RuleMatcher(args.rules)
        hit = matcher.match(asr_text_trad)
        
        if not hit:
            # å¦‚æœæ²’æœ‰å‘½ä¸­è¦å‰‡ï¼Œä¸”è™•æ–¼æ§åˆ¶æ¨¡å¼ï¼Œè¿”å›å›ºå®šå¼•å°èª
            if mode_manager and mode_manager.is_control_mode():
                reply_text = mode_manager.get_mismatch_reply()
                print("âš ï¸ æ§åˆ¶æ¨¡å¼ä¸‹è¦å‰‡æœªå‘½ä¸­ï¼Œä½¿ç”¨å›ºå®šå¼•å°èª")
                print("\n==== å›ºå®šå¼•å°èª ====")
                print(reply_text)
                print("==================\n")
                return reply_text, args.voice
            return None
        
        # è¼‰å…¥è¦å‰‡è³‡æ–™ä»¥å–å¾—å…¨åŸŸè¨­å®š
        rules_data = load_rules(args.rules)
        
    except Exception as e:
        print("âš ï¸ è¦å‰‡æª”è®€å–/æ¯”å°å¤±æ•—ï¼š", e)
        return None
    
    if app_config.low_latency_mode:
        show_fast_progress("âš™ï¸ è™•ç†è¦å‰‡å›è¦†")
    else:
        show_progress("âš™ï¸ è™•ç†è¦å‰‡å›è¦†", 0.3)
    
    # å¯æ³¨å…¥å‹•æ…‹ contextï¼ˆä¾‹ï¼šå‰©é¤˜çƒæ•¸ã€ç›®å‰é€Ÿåº¦ç­‰ï¼‰
    context = {
        "balls_left": 48,    # â† é€™è£¡æ¥åˆ°ä½ çš„ç³»çµ±ç‹€æ…‹
        # "speed": current_speed,
    }
    reply_text = format_reply(hit.get("reply", {}).get("text", ""), context)
    voice = hit.get("reply", {}).get("voice", rules_data.get("globals", {}).get("default_voice", args.voice)) or args.voice

    print(f"âœ… å‘½ä¸­è¦å‰‡ï¼š{hit.get('id','(no-id)')}  action={hit.get('action','')}")
    print("\n==== å›ºå®šå›è¦† ====")
    print(reply_text)
    print("==================\n")

    # å¿«å–è¦å‰‡å›è¦†çµæœ
    if app_config.preload.enabled:
        reply_cache.cache_reply(asr_text_trad, reply_text)

    # é€™è£¡å¯å°æ¥ä½ çš„å¯¦é«”å‹•ä½œï¼ˆç™¼çƒæ©Ÿ APIï¼‰
    # do_action(hit.get("action"), context)
    
    return reply_text, voice


def _handle_wake_word(asr_text_trad: str, args: argparse.Namespace) -> Optional[str]:
    """è™•ç†å–šé†’è©ï¼Œè¿”å›å›è¦†æ–‡å­—æˆ– None"""
    if not is_wake_hit(asr_text_trad, args.wake):
        return None
    
    reply_text = args.wake_reply
    print(f"ğŸ”” å–šé†’è©å‘½ä¸­ï¼š{args.wake} â†’ ç›´æ¥å›è¦†")
    print("\n==== å›ºå®šå›è¦† ====")
    print(reply_text)
    print("==================\n")
    
    return reply_text


def _handle_llm_response(client: OpenAI, asr_text_trad: str, args: argparse.Namespace, 
                        conversation_history: Optional[list], preload_manager: Optional[PreloadManager] = None,
                        mode_manager: Optional['ModeManager'] = None) -> str:
    """è™•ç† LLM å›æ‡‰ï¼ˆæ”¯æ´é è¼‰å…¥å¿«å–å’Œæ¨¡å¼åˆ†æµï¼‰"""
    
    # æª¢æŸ¥æ˜¯å¦è™•æ–¼æ§åˆ¶æ¨¡å¼ï¼Œå¦‚æœæ˜¯å‰‡ä¸æ‡‰è©²é€²å…¥ LLM è™•ç†
    if mode_manager and mode_manager.is_control_mode():
        print("âš ï¸ æ§åˆ¶æ¨¡å¼ä¸‹ä¸æ‡‰é€²å…¥ LLM è™•ç†ï¼Œé€™è¡¨ç¤ºé‚è¼¯æœ‰èª¤")
        return mode_manager.get_mismatch_reply()
    
    # é¦–å…ˆæª¢æŸ¥å¿«å–
    cached_reply = reply_cache.get_cached_reply(asr_text_trad)
    if cached_reply:
        print("âš¡ ä½¿ç”¨å¿«å–å›è¦†")
        print("\n==== å¿«å–å›è¦†ï¼ˆæ–‡å­—ï¼‰====")
        print(cached_reply)
        print("=======================\n")
        
        # é æ¸¬å¾ŒçºŒå¯èƒ½çš„å•é¡Œ
        if preload_manager:
            reply_cache.predict_and_preload(asr_text_trad, conversation_history or [])
        
        return cached_reply
    
    # æª¢æŸ¥å¸¸ç”¨å›è¦†æ¨¡æ¿
    common_reply = reply_cache.get_common_reply(asr_text_trad)
    if common_reply:
        print("ğŸ“‹ ä½¿ç”¨å¸¸ç”¨å›è¦†æ¨¡æ¿")
        print("\n==== æ¨¡æ¿å›è¦†ï¼ˆæ–‡å­—ï¼‰====")
        print(common_reply)
        print("=======================\n")
        
        # å¿«å–é€™å€‹å›è¦†
        reply_cache.cache_reply(asr_text_trad, common_reply)
        
        # é æ¸¬å¾ŒçºŒå¯èƒ½çš„å•é¡Œ
        if preload_manager:
            reply_cache.predict_and_preload(asr_text_trad, conversation_history or [])
        
        return common_reply
    
    # ä½¿ç”¨ LLM ç”Ÿæˆæ–°å›è¦†
    if app_config.low_latency_mode:
        show_fast_progress("ğŸ¤– æº–å‚™ LLM è«‹æ±‚")
    else:
        show_progress("ğŸ¤– æº–å‚™ LLM è«‹æ±‚", 0.2)
    
    # è‹¥å•Ÿç”¨ç°¡æ½”æ¨¡å¼ï¼Œè‡ªå‹•åœ¨ system æŒ‡ç¤ºåŠ å…¥é™åˆ¶å­—æ•¸/å¥æ•¸èˆ‡å£å»
    system_prompt = args.system
    if args.concise:
        concise_rule = (
            "è«‹ç”¨æœ€çŸ­çš„æ–¹å¼å›ç­”ï¼Œ 1~2 å¥æˆ– 20 å­—ä»¥å…§"
            "å¹½é»˜ä¸€é»ä¸è¦å¤ªæ­£ç¶“ï¼Œåƒå€‹å¤§å­¸ç”ŸèŠå¤©"
            "- ç›´å…¥é‡é»ï¼Œé¿å…è´…è¿°èˆ‡åˆ—é»ã€‚"
        )
        system_prompt = (system_prompt + "\n" + concise_rule).strip() if system_prompt else concise_rule

    if app_config.low_latency_mode:
        show_fast_progress("ğŸ§  ç”Ÿæˆ LLM å›è¦†")
    else:
        show_progress_with_dots("ğŸ§  ç”Ÿæˆ LLM å›è¦†", 4)
    
    reply = llm_reply_with_retry(
        client,
        asr_text_trad,
        system_prompt,
        temperature=args.temperature,
        max_tokens=args.max_tokens,
        conversation_history=conversation_history,
    )
    
    # å¿«å–æ–°ç”Ÿæˆçš„å›è¦†
    reply_cache.cache_reply(asr_text_trad, reply)
    
    # é æ¸¬å¾ŒçºŒå¯èƒ½çš„å•é¡Œ
    if preload_manager:
        reply_cache.predict_and_preload(asr_text_trad, conversation_history or [])
    
    print("\n==== LLM å›è¦†ï¼ˆæ–‡å­—ï¼‰====")
    print(reply)
    print("=======================\n")
    
    return reply


def _handle_tts_output(client: OpenAI, reply_text: str, voice: str, args: argparse.Namespace) -> None:
    """è™•ç† TTS è¼¸å‡º"""
    if app_config.low_latency_mode:
        show_fast_progress(f"ğŸ”Š è½‰ç‚ºèªéŸ³ä¸­ (èªé€Ÿ: {args.speed}x)")
    else:
        show_progress_with_dots(f"ğŸ”Š è½‰ç‚ºèªéŸ³ä¸­ (èªé€Ÿ: {args.speed}x)", 3)
    
    tts_speak_with_retry(client, reply_text, voice, args.output, args.speed)
    print(f"âœ… å·²ç”¢ç”ŸèªéŸ³æª”ï¼š{args.output}")


def _update_conversation_history(conversation_history: Optional[list], 
                                asr_text_trad: str, reply: str) -> list:
    """æ›´æ–°å°è©±æ­·å²"""
    updated_history = conversation_history or []
    updated_history.append({"role": "user", "content": asr_text_trad})
    updated_history.append({"role": "assistant", "content": reply})
    
    # é™åˆ¶å°è©±æ­·å²é•·åº¦ï¼ˆä¿ç•™æœ€è¿‘ N è¼ªå°è©±ï¼‰
    max_history = app_config.max_conversation_history * 2  # æ¯è¼ªå°è©± = 2 æ¢è¨Šæ¯
    if len(updated_history) > max_history:
        updated_history = updated_history[-max_history:]
    
    return updated_history


def run_once(args: argparse.Namespace, client: OpenAI, conversation_history: Optional[list] = None, 
             preload_manager: Optional[PreloadManager] = None, mode_manager: Optional['ModeManager'] = None) -> tuple[str, list]:
    """ä¸»è¦åŸ·è¡Œæµç¨‹ï¼Œå”èª¿å„å€‹å­å‡½æ•¸ï¼ˆæ”¯æ´æ¨¡å¼åˆ†æµï¼‰"""
    # _log_memory_usage("æµç¨‹é–‹å§‹")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„
    
    # 1) æº–å‚™è¼¸å…¥éŸ³æª”
    wav_path, asr_text = _prepare_audio_input(args)
    # _log_memory_usage("éŸ³è¨Šæº–å‚™å®Œæˆ")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„

    # 2) è™•ç†èªéŸ³è­˜åˆ¥
    asr_text_trad = _process_asr(client, wav_path, asr_text, args)
    # _log_memory_usage("èªéŸ³è­˜åˆ¥å®Œæˆ")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„

    # ç©ºè¼¸å…¥å°±ä¸ç¹¼çºŒ
    if not asr_text_trad.strip():
        print("âš ï¸ ç„¡å…§å®¹ï¼Œç•¥é LLM èˆ‡ TTSã€‚")
        return "", conversation_history or []

    # 2.5) æª¢æŸ¥æ¨¡å¼åˆ‡æ›ï¼ˆå„ªå…ˆè™•ç†ï¼‰
    if mode_manager:
        mode_switch_reply = mode_manager.check_mode_switch(asr_text_trad)
        if mode_switch_reply:
            _handle_tts_output(client, mode_switch_reply, args.voice, args)
            autoplay_mac(args.output, enabled=not args.no_play)
            print(f"ğŸ”„ ç•¶å‰æ¨¡å¼ï¼š{mode_manager.get_current_mode()}")
            return mode_switch_reply, conversation_history or []

    # 3) è¦å‰‡åŒ¹é…ï¼ˆæ”¯æ´æ¨¡å¼åˆ†æµï¼‰
    rules_result = _handle_rules_matching(asr_text_trad, args, mode_manager)
    if rules_result:
        reply_text, voice = rules_result
        _handle_tts_output(client, reply_text, voice, args)
        autoplay_mac(args.output, enabled=not args.no_play)
        # _log_memory_usage("è¦å‰‡åŒ¹é…å®Œæˆ")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„
        return reply_text, conversation_history or []

    # 4) å–šé†’è©è™•ç†
    wake_reply = _handle_wake_word(asr_text_trad, args)
    if wake_reply:
        _handle_tts_output(client, wake_reply, args.voice, args)
        autoplay_mac(args.output, enabled=not args.no_play)
        # _log_memory_usage("å–šé†’è©è™•ç†å®Œæˆ")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„
        return wake_reply, conversation_history or []

    # 5) LLM å›è¦†ï¼ˆæ”¯æ´é è¼‰å…¥å’Œæ¨¡å¼åˆ†æµï¼‰
    # åªæœ‰åœ¨æ€è€ƒæ¨¡å¼ä¸‹æ‰ä½¿ç”¨ LLM
    if mode_manager and mode_manager.is_control_mode():
        # æ§åˆ¶æ¨¡å¼ä¸‹ä¸æ‡‰è©²åˆ°é”é€™è£¡ï¼Œå› ç‚ºè¦å‰‡åŒ¹é…æ‡‰è©²å·²ç¶“è™•ç†äº†
        print("âš ï¸ æ§åˆ¶æ¨¡å¼ä¸‹ä¸æ‡‰é€²å…¥ LLM è™•ç†ï¼Œä½¿ç”¨å›ºå®šå¼•å°èª")
        reply = mode_manager.get_mismatch_reply()
    else:
        reply = _handle_llm_response(client, asr_text_trad, args, conversation_history, preload_manager, mode_manager)
    # _log_memory_usage("LLM å›è¦†å®Œæˆ")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„

    # 6) TTS è¼¸å‡º
    _handle_tts_output(client, reply, args.voice, args)
    # _log_memory_usage("TTS è¼¸å‡ºå®Œæˆ")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„

    # 7) è‡ªå‹•æ’­æ”¾
    autoplay_mac(args.output, enabled=not args.no_play)
    
    # 8) æ›´æ–°å°è©±æ­·å²
    updated_history = _update_conversation_history(conversation_history, asr_text_trad, reply)
    # _log_memory_usage("æµç¨‹å®Œæˆ")  # å·²åœç”¨è¨˜æ†¶é«”è¨˜éŒ„
    
    return reply, updated_history


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="ğŸ¸ ç¾½çƒç™¼çƒæ©ŸèªéŸ³æ§åˆ¶ç³»çµ±")
    g_in = parser.add_mutually_exclusive_group()
    g_in.add_argument("-d", "--duration", type=int, default=None, help="éŒ„éŸ³ç§’æ•¸ï¼ˆèˆ‡ -i å’Œ --vad äº’æ–¥ï¼‰")
    g_in.add_argument("-i", "--input", type=str, default=None, help="è¼¸å…¥éŸ³æª”è·¯å¾‘ï¼ˆ.wav/.mp3/.m4a ç­‰ï¼‰")
    g_in.add_argument("--vad", action="store_true", default=True, help="å•Ÿç”¨ VAD æ¨¡å¼ï¼Œè‡ªå‹•åµæ¸¬èªéŸ³çµæŸï¼ˆèˆ‡ -d å’Œ -i äº’æ–¥ï¼‰")
    g_in.add_argument("--realtime", action="store_true", help="å•Ÿç”¨å³æ™‚è½‰éŒ„æ¨¡å¼ï¼ˆå°‡åœ¨éŒ„éŸ³å®Œæˆå¾Œè½‰éŒ„ï¼‰")

    parser.add_argument("-o", "--output", type=str, default="demo.mp3", help="è¼¸å‡º mp3 æª”å")
    parser.add_argument("-v", "--voice", type=str, default="alloy", help="TTS èªè€…ï¼šalloy/echo/fable/onyx/nova/shimmer")
    parser.add_argument("--system", type=str, default="", help="å¯é¸çš„ç³»çµ±å‰æ/å£å»æŒ‡ç¤ºï¼ˆå¯æ­é… --conciseï¼‰")
    parser.add_argument("--tmp", type=str, default="input.wav", help="éŒ„éŸ³æš«å­˜æª”è·¯å¾‘")
    parser.add_argument("--no-s2twp", action="store_true", help="åœç”¨ç°¡è½‰ç¹ï¼ˆs2twpï¼‰")
    parser.add_argument("--no-play", action="store_true", help="ç”¢ç”ŸéŸ³æª”ä½†ä¸è‡ªå‹•æ’­æ”¾")
    parser.add_argument("--loop", action="store_true", help="å¤šå›åˆäº’å‹•æ¨¡å¼ï¼ˆé è¨­å•Ÿç”¨ï¼‰")
    parser.add_argument("--no-loop", action="store_true", help="åœç”¨å¤šå›åˆæ¨¡å¼ï¼ŒåªåŸ·è¡Œä¸€æ¬¡")
    parser.add_argument("--continuous", action="store_true", help="æŒçºŒå°è©±æ¨¡å¼ï¼ˆè‡ªå‹•å¾ªç’°ï¼Œæ”¯æ´ä¸Šä¸‹æ–‡è¨˜æ†¶ï¼‰")
    parser.add_argument("--auto-restart", action="store_true", help="æŒçºŒæ¨¡å¼ä¸­è‡ªå‹•é‡æ–°é–‹å§‹éŒ„éŸ³ï¼ˆç„¡éœ€æŒ‰ Enterï¼‰")
    parser.add_argument("--sd-device", type=int, default=None, help="sounddevice è¼¸å…¥è£ç½®ç´¢å¼•")
    parser.add_argument("--adev", type=int, default=0, help="ffmpeg avfoundation éŸ³è¨Šè£ç½®ç´¢å¼•ï¼Œé è¨­ 0")
    parser.add_argument("--concise", action="store_true", help="å•Ÿç”¨ç°¡æ½”å›ç­”æ¨¡å¼ï¼ˆ1~2 å¥å…§ï¼Œå°‘å»¢è©±ï¼‰")
    parser.add_argument("--temperature", type=float, default=0.5, help="LLM æº«åº¦ï¼ˆ0~2ï¼Œè¶Šä½è¶Šä¿å®ˆï¼‰")
    parser.add_argument("--max-tokens", type=int, default=120, help="é™åˆ¶å›è¦†æœ€å¤§ tokens æ•¸")
    parser.add_argument("--wake", type=str, default=DEFAULT_WAKE, help="å–šé†’è©ï¼Œå‘½ä¸­æ™‚ç›´æ¥å›è¦†å›ºå®šå¥")
    parser.add_argument("--wake-reply", type=str, default=DEFAULT_WAKE_REPLY, help="å–šé†’è©å‘½ä¸­æ™‚çš„å›ºå®šå›è¦†")
    parser.add_argument("--speed", type=float, default=1.2, help="TTS èªé€Ÿå€ç‡ï¼ˆ1.0=æ­£å¸¸ï¼Œ1.2=é è¨­1.2å€é€Ÿï¼Œ1.5=1.5å€é€Ÿï¼Œ2.0=2å€é€Ÿï¼‰")
    parser.add_argument("--rules", type=str, default="rules/badminton_rules.yaml", help="è¦å‰‡æª”è·¯å¾‘ï¼ˆYAMLï¼‰ã€‚è¨­å®šå¾Œå°‡å…ˆåšè¦å‰‡åŒ¹é…ï¼›å‘½ä¸­å‰‡è·³é LLM")
    parser.add_argument("--no-rules", action="store_true", help="å¿½ç•¥è¦å‰‡æª”ï¼ˆé™¤éŒ¯ç”¨ï¼‰")
    
    # èªéŸ³è­˜åˆ¥åƒæ•¸
    parser.add_argument("--whisper-model", type=str, default="whisper-1", help="Whisper æ¨¡å‹ï¼šwhisper-1")
    parser.add_argument("--whisper-language", type=str, default="zh", help="Whisper èªè¨€ä»£ç¢¼ï¼šzhï¼ˆä¸­æ–‡ï¼‰ã€enï¼ˆè‹±æ–‡ï¼‰ã€jaï¼ˆæ—¥æ–‡ï¼‰ç­‰")
    
    # ä½å»¶é²å„ªåŒ–åƒæ•¸
    parser.add_argument("--low-latency", action="store_true", help="å•Ÿç”¨ä½å»¶é²æ¨¡å¼ï¼ˆæ¸›å°‘ VAD ç­‰å¾…æ™‚é–“ï¼Œè·³éé€²åº¦æŒ‡ç¤ºå™¨ï¼‰")
    parser.add_argument("--ultra-fast", action="store_true", help="å•Ÿç”¨è¶…å¿«é€Ÿæ¨¡å¼ï¼ˆæœ€ä½å»¶é²ï¼Œå¯èƒ½å½±éŸ¿æº–ç¢ºæ€§ï¼‰")
    parser.add_argument("--no-progress", action="store_true", help="è·³éæ‰€æœ‰é€²åº¦æŒ‡ç¤ºå™¨")
    parser.add_argument("--parallel", action="store_true", help="å•Ÿç”¨ä¸¦è¡Œè™•ç†ï¼ˆå¯¦é©—æ€§åŠŸèƒ½ï¼‰")
    
    # é è¼‰å…¥å„ªåŒ–åƒæ•¸
    parser.add_argument("--preload", action="store_true", help="å•Ÿç”¨é è¼‰å…¥å›è¦†æ¨¡æ¿ç³»çµ±")
    parser.add_argument("--no-preload", action="store_true", help="åœç”¨é è¼‰å…¥ç³»çµ±")
    parser.add_argument("--preload-common", action="store_true", help="é è¼‰å…¥å¸¸ç”¨å›è¦†æ¨¡æ¿")
    parser.add_argument("--cache-stats", action="store_true", help="é¡¯ç¤ºå¿«å–çµ±è¨ˆè³‡è¨Š")
    parser.add_argument("--no-persistent-cache", action="store_true", help="åœç”¨æŒä¹…åŒ–å¿«å–")
    parser.add_argument("--save-cache", action="store_true", help="ç«‹å³å„²å­˜å¿«å–")
    parser.add_argument("--clear-cache", action="store_true", help="æ¸…ç©ºå¿«å–")
    # è¦å‰‡å¿«å–åƒæ•¸
    parser.add_argument("--no-rule-cache", action="store_true", help="åœç”¨è¦å‰‡å¿«å–")
    parser.add_argument("--no-preload-rules", action="store_true", help="åœç”¨è¦å‰‡é è¼‰å…¥")
    parser.add_argument("--rule-cache-ttl", type=int, default=300, help="è¦å‰‡å¿«å–å­˜æ´»æ™‚é–“ï¼ˆç§’ï¼‰")
    
    # æ¨¡å¼åˆ†æµåƒæ•¸
    parser.add_argument("--default-mode", choices=["control","think"], default="control",
                        help="å•Ÿå‹•é è¨­æ¨¡å¼ï¼šcontrol=æ§åˆ¶æ¨¡å¼(ç„¡LLM)ã€think=æ€è€ƒæ¨¡å¼(å…è¨±LLM)")
    parser.add_argument("--think-on", type=str, default="å•Ÿå‹•æ€è€ƒæ¨¡å¼",
                        help="åˆ‡æ›åˆ°æ€è€ƒæ¨¡å¼çš„é—œéµå­—")
    parser.add_argument("--control-on", type=str, default="å•Ÿå‹•æ§åˆ¶æ¨¡å¼",
                        help="åˆ‡å›æ§åˆ¶æ¨¡å¼çš„é—œéµå­—")
    parser.add_argument("--mismatch-reply", type=str, default="æˆ‘ç¾åœ¨åœ¨æ§åˆ¶æ¨¡å¼ï¼Œè«‹ç”¨æ˜ç¢ºçš„æŒ‡ä»¤å†èªªä¸€æ¬¡ã€‚",
                        help="æ§åˆ¶æ¨¡å¼ä¸‹è¦å‰‡ä¸åŒ¹é…æ™‚çš„å›è¦†")
    
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    
    # æ‡‰ç”¨ä½å»¶é²é…ç½®
    if args.low_latency or args.ultra_fast:
        app_config.low_latency_mode = True
        print("âš¡ ä½å»¶é²æ¨¡å¼å·²å•Ÿç”¨")
    
    if args.ultra_fast:
        app_config.audio.silence_ms = app_config.audio.ultra_fast_silence_ms
        app_config.audio.min_speech_frames = app_config.audio.min_speech_frames_fast
        print("ğŸš€ è¶…å¿«é€Ÿæ¨¡å¼å·²å•Ÿç”¨")
    
    if args.no_progress:
        app_config.skip_progress_indicators = True
        print("ğŸ“Š é€²åº¦æŒ‡ç¤ºå™¨å·²åœç”¨")
    
    if args.parallel:
        app_config.parallel_processing = True
        print("ğŸ”„ ä¸¦è¡Œè™•ç†å·²å•Ÿç”¨")
    
    # æ‡‰ç”¨é è¼‰å…¥é…ç½®
    if args.no_preload:
        app_config.preload.enabled = False
        print("ğŸš« é è¼‰å…¥ç³»çµ±å·²åœç”¨")
    elif args.preload:
        app_config.preload.enabled = True
        print("ğŸ“‹ é è¼‰å…¥ç³»çµ±å·²å•Ÿç”¨")
    
    # æ‡‰ç”¨æŒä¹…åŒ–å¿«å–é…ç½®
    if args.no_persistent_cache:
        app_config.preload.persistent_cache = False
        print("ğŸš« æŒä¹…åŒ–å¿«å–å·²åœç”¨")
    
    # æ‡‰ç”¨è¦å‰‡å¿«å–é…ç½®
    if args.no_rule_cache:
        app_config.preload.rule_cache_enabled = False
        print("ğŸš« è¦å‰‡å¿«å–å·²åœç”¨")
    
    if args.no_preload_rules:
        app_config.preload.preload_rules = False
        print("ğŸš« è¦å‰‡é è¼‰å…¥å·²åœç”¨")
    
    if args.rule_cache_ttl != 300:
        app_config.preload.rule_cache_ttl = args.rule_cache_ttl
        print(f"â° è¦å‰‡å¿«å–å­˜æ´»æ™‚é–“è¨­ç‚º {args.rule_cache_ttl} ç§’")
    
    # åˆå§‹åŒ–æ—¥èªŒç³»çµ±
    setup_logging("INFO")

    # æª¢æŸ¥é‡‘é‘°å†åˆå§‹åŒ– Client
    if os.environ.get("OPENAI_API_KEY") in (None, "", "ä½ çš„key"):
        print("âŒ è«‹å…ˆè¨­å®šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY")
        sys.exit(1)
    client = OpenAI()
    
    # åˆå§‹åŒ–æ¨¡å¼ç®¡ç†å™¨
    mode_manager = ModeManager(
        default_mode=args.default_mode,
        think_on=args.think_on,
        control_on=args.control_on,
        mismatch_reply=args.mismatch_reply
    )
    print(f"ğŸ›ï¸ æ¨¡å¼ç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼Œé è¨­æ¨¡å¼ï¼š{mode_manager.get_current_mode()}")
    print(f"   - åˆ‡æ›åˆ°æ€è€ƒæ¨¡å¼ï¼š{args.think_on}")
    print(f"   - åˆ‡æ›åˆ°æ§åˆ¶æ¨¡å¼ï¼š{args.control_on}")

    # åˆå§‹åŒ–é è¼‰å…¥ç®¡ç†å™¨
    preload_manager = None
    if app_config.preload.enabled:
        preload_manager = PreloadManager(client)
        preload_manager.start_background_preload()
        
        if args.preload_common:
            preload_manager.preload_common_queries()
    
    # è™•ç†å¿«å–ç®¡ç†å‘½ä»¤
    if args.save_cache:
        reply_cache.save_cache_now()
        return
    
    if args.clear_cache:
        reply_cache.clear_cache()
        return
    
    # é¡¯ç¤ºå¿«å–çµ±è¨ˆ
    if args.cache_stats:
        stats = reply_cache.get_cache_stats()
        print(f"ğŸ“Š å¿«å–çµ±è¨ˆï¼š{stats}")
        return

    # é è¨­ç‚ºæŒçºŒå°è©±æ¨¡å¼ï¼Œé™¤éæ˜ç¢ºæŒ‡å®šå…¶ä»–æ¨¡å¼
    if args.no_loop and not args.continuous and not args.loop:
        reply, _ = run_once(args, client, preload_manager=preload_manager, mode_manager=mode_manager)
        return
    
    # å¦‚æœæ²’æœ‰æŒ‡å®šä»»ä½•æ¨¡å¼ï¼Œé è¨­é€²å…¥æŒçºŒå°è©±æ¨¡å¼
    if not args.loop and not args.continuous:
        args.continuous = True

    # æŒçºŒå°è©±æ¨¡å¼ï¼ˆé è¨­ï¼‰
    if args.continuous:
        print("ğŸ”„ é€²å…¥æŒçºŒå°è©±æ¨¡å¼ã€‚æ”¯æ´ä¸Šä¸‹æ–‡è¨˜æ†¶ï¼ŒCtrl+C é›¢é–‹ã€‚")
        if args.auto_restart:
            print("âš¡ è‡ªå‹•é‡å•Ÿæ¨¡å¼ï¼šéŒ„éŸ³çµæŸå¾Œè‡ªå‹•é–‹å§‹ä¸‹ä¸€è¼ª")
        else:
            print("â¸ï¸ æ‰‹å‹•æ¨¡å¼ï¼šæ¯è¼ªçµæŸå¾ŒæŒ‰ Enter ç¹¼çºŒ")
        
        conversation_history = []
        round_count = 0
        
        try:
            while True:
                round_count += 1
                print(f"\n{'='*50}")
                print(f"ğŸ¯ ç¬¬ {round_count} è¼ªå°è©±")
                print(f"ğŸ›ï¸ ç•¶å‰æ¨¡å¼ï¼š{mode_manager.get_current_mode()}")
                print(f"{'='*50}")
                
                reply, conversation_history = run_once(args, client, conversation_history, preload_manager, mode_manager)
                
                if args.auto_restart:
                    print("â³ 1 ç§’å¾Œè‡ªå‹•é–‹å§‹ä¸‹ä¸€è¼ª...")
                    import time
                    time.sleep(1)
                else:
                    input("ï¼ˆæŒ‰ Enter ç¹¼çºŒä¸‹ä¸€è¼ªï¼Œæˆ– Ctrl+C çµæŸï¼‰")
                    
        except KeyboardInterrupt:
            print(f"\nğŸ‘‹ å·²çµæŸã€‚ç¸½å…±é€²è¡Œäº† {round_count} è¼ªå°è©±ã€‚")
            # é¡¯ç¤ºæœ€çµ‚å¿«å–çµ±è¨ˆ
            if app_config.preload.enabled:
                stats = reply_cache.get_cache_stats()
                print(f"ğŸ“Š æœ€çµ‚å¿«å–çµ±è¨ˆï¼š{stats}")
        finally:
            # æ¸…ç†é è¼‰å…¥ç®¡ç†å™¨
            if preload_manager:
                preload_manager.stop_background_preload()
            # å„²å­˜å¿«å–
            if app_config.preload.enabled and app_config.preload.persistent_cache:
                reply_cache.save_cache_now()
        return

    # å‚³çµ±å¤šå›åˆæ¨¡å¼
    print("ğŸ” é€²å…¥å¤šå›åˆæ¨¡å¼ã€‚æ¯å›åˆçµæŸå¾ŒæŒ‰ Enter é€²å…¥ä¸‹ä¸€å›åˆï¼ŒCtrl+C é›¢é–‹ã€‚")
    try:
        while True:
            reply, _ = run_once(args, client, preload_manager=preload_manager, mode_manager=mode_manager)
            input("ï¼ˆæŒ‰ Enter ç¹¼çºŒä¸‹ä¸€å›åˆï¼Œæˆ– Ctrl+C çµæŸï¼‰")
    except KeyboardInterrupt:
        print("\nğŸ‘‹ å·²çµæŸã€‚")
        # é¡¯ç¤ºæœ€çµ‚å¿«å–çµ±è¨ˆ
        if app_config.preload.enabled:
            stats = reply_cache.get_cache_stats()
            print(f"ğŸ“Š æœ€çµ‚å¿«å–çµ±è¨ˆï¼š{stats}")
    finally:
        # æ¸…ç†é è¼‰å…¥ç®¡ç†å™¨
        if preload_manager:
            preload_manager.stop_background_preload()
        # å„²å­˜å¿«å–
        if app_config.preload.enabled and app_config.preload.persistent_cache:
            reply_cache.save_cache_now()


if __name__ == "__main__":
    main()
